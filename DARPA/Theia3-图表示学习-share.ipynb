{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# msg的特征采取    [src_node_feature,edge_attr,dst_node_feature]的格式\n",
    "\n",
    "# compute the best partition 计算出最佳的社区划分\n",
    "import datetime\n",
    "import community as community_louvain\n",
    "\n",
    "import xxhash\n",
    "# 查找edge向量所对应的下标\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "# 单独计算出每条边的loss值来分析出 具体的异常行为\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    loss=[]\n",
    "    for i in link_pred_ratio:\n",
    "        loss.append(criterion(i,torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[] \n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pos_edges_loss_autoencoder(decoded,msg):\n",
    "    loss=[] \n",
    "    for i in range(len(decoded)):\n",
    "        loss.append(criterion(decoded[i].reshape(-1),msg[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间戳转换函数\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int 秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找edge向量所对应的下标\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CONNECT',\n",
    " 'EVENT_CONNECT': 1,\n",
    " 2: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 2,\n",
    " 3: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 3,\n",
    " 4: 'EVENT_READ',\n",
    " 'EVENT_READ': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8,\n",
    " 9: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接postgessql 数据库，将数据存入数据库中\n",
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_theia_dataset_db',\n",
    "                           user = 'postgres',\n",
    "                           password = 'Ssys8.21',\n",
    "                           port = '5432'#一般是5432\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "# 创建一个cursor来执行数据库的操作\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 构建nodeid 与msg之间的构建  创建字典变量 nodeid2msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # 可以实现 nodeid 转 msg      node hash 转 nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  #0-828297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载对应的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_3=torch.load(\"./train_graph/***\")\n",
    "graph_4_4=torch.load(\"./train_graph/***\")\n",
    "graph_4_5=torch.load(\"./train_graph/***\")\n",
    "graph_4_9=torch.load(\"./train_graph/***\")\n",
    "\n",
    "graph_4_10=torch.load(\"./train_graph/***\")\n",
    "graph_4_11=torch.load(\"./train_graph/***\")\n",
    "graph_4_12=torch.load(\"./train_graph/***\")\n",
    "graph_4_13=torch.load(\"./train_graph/***\")\n",
    "train_data=graph_4_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node_num = 828398 \n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=20, device=device)\n",
    "\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(in_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        \n",
    "        self.lin_seq = nn.Sequential(\n",
    "            \n",
    "            Linear(in_channels*4, in_channels*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*8, in_channels*2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*2, int(in_channels//2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels//2), train_data.msg.shape[1]-32)                   \n",
    "        )\n",
    "        \n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "         \n",
    "        h = self.lin_seq (h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "# 更改节点嵌入向量维度\n",
    "memory_dim = time_dim = embedding_dim = 100\n",
    "\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "saved_nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1024\n",
    "def train(train_data):\n",
    "\n",
    "    \n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_batchs=len(train_data.t)//BATCH +1\n",
    "    batch_index=0\n",
    "#     print(\"train_before_stage_data:\",train_data)\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        start = time.perf_counter()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "        \n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "#         n_id = torch.cat([src, pos_dst, neg_src, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        \n",
    "        print(f\"{n_id=}\")\n",
    "        print(f\"{edge_index=}\")\n",
    "        print(f\"{train_data.msg[e_id]=}\")\n",
    "      \n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        \n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        \n",
    "#         y_true = torch.cat([torch.zeros(pos_out.size(0),1),torch.ones(neg_out.size(0),1)], dim=0)# 0 代表正常 1 代表异常\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "          \n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "# 原先代码中的loss计算方法\n",
    "#         loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "#         loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "#         for i in range(len(src)):\n",
    "#             saved_nodes.add(int(src[i]))\n",
    "#             saved_nodes.add(int(pos_dst[i]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "#         print(z.shape)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "        batch_index+=1\n",
    "        end = time.perf_counter()\n",
    "        print(f\"current epoch process:{(batch_index/total_batchs):.4f}%   cost time:{(end-start):.2f}\")\n",
    "#     print(\"trained_stage_data:\",train_data)\n",
    "    return total_loss / train_data.num_events`````````````````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs=[\n",
    "    graph_4_3,\n",
    "    graph_4_4, \n",
    "    graph_4_5,\n",
    "#     graph_4_9\n",
    "]\n",
    "\n",
    "for epoch in tqdm(range(1, 31)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "#     scheduler.step()\n",
    "    # 将训练好的模型保存下来\n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "torch.save(model,\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass_without_neg_edge.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# 检查点的时间是否设置长一些会更好呢？  比如说十分钟？\n",
    "#  300000000000  是五分钟的纳秒时间段\n",
    "# 测试一天的数据，每五分钟给出一个loss值  每五分钟给出一个checkpoint 的loss值\n",
    "@torch.no_grad()#声明以下函数不执行梯度\n",
    "def test_day_new_without_negative(inference_data,path):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "#     m=torch.load(\"model_saved_emb100.pt\")\n",
    "#     memory,gnn, link_pred,neighbor_loader=m\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  # Start with a fresh memory.  # 为什么不可以使用历史的memory呢？  应该 可以吧？\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    \n",
    "    time_with_loss={}# key: 时间段，  value： 该时间段的loss值\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([])\n",
    "    total_edges=0\n",
    "    \n",
    "#     test_memory=copy.deepcopy(memory)   \n",
    "#     test_gnn=copy.deepcopy(gnn)   \n",
    "#     test_link_pred=copy.deepcopy(link_pred) \n",
    "#     test_neighbor_loader=copy.deepcopy(neighbor_loader)\n",
    "\n",
    "# 记录起始的时间点\n",
    "\n",
    "    start_time=inference_data.t[0]\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    \n",
    "    loss_list=[]\n",
    "    \n",
    "#     print(\"before merge:\",train_data)\n",
    "\n",
    "#     nique_node_count=len(torch.cat([train_data.src,train_data.dst]).unique())\n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "    # 记录程序运行时间  评估运行效率\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        \n",
    "       \n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        #如果memory和neighbor_loader在测试阶段都没有被更新的话，此处不需要采用gnn获得z矩阵，只需要从memory查出来对应的节点特征向量即可\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        \n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "#         y_true = torch.cat(\n",
    "#             [torch.ones(pos_out.size(0))], dim=0).to(torch.long)     \n",
    "#         y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        # 只考虑边有没有被正确预测，对于正常行为的图而言，行为模式比较相似所以loss较低。  对于异常行为，会存在一些行为没见过，所以对这些行为预测存在的概率就地，所以loss也会高。\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "     \n",
    "        \n",
    "# 将当前batch 发生的edge 更新到memory 和neighbor中\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        #计算每条边的loss值\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode=int(src[i])\n",
    "            dstnode=int(pos_dst[i])  \n",
    "            \n",
    "            srcmsg=str(nodeid2msg[srcnode]) \n",
    "            dstmsg=str(nodeid2msg[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1) # find 找出来的范围是 1-n   rel2id中的id也是1-n    \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "            \n",
    "            # 先不考虑与socket 节点 当找出可疑的进程与文件之后再将socket找出\n",
    "#             if \"netflow\" in srcmsg or \"netflow\" in dstmsg:\n",
    "#                 temp_dic['loss']=0\n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000000000*15:\n",
    "            # 此处为一个checkpoint  输出loss值 并且清空全局的loss值  保存处理过的节点\n",
    "#             loss=total_loss/event_count\n",
    "            time_interval=ns_time_to_datetime_US(start_time)+\"~\"+ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                \n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "            # 减去train data中没有被训练好的\n",
    "            \n",
    "            for e in edge_list: \n",
    "#                 temp_key=e['srcmsg']+e['dstmsg']+e['edge_type']\n",
    "#                 if temp_key in train_edge_set:      \n",
    "# #                     e['loss']=(e['loss']-train_edge_set[temp_key]) if e['loss']>=train_edge_set[temp_key] else 0  \n",
    "# #                     e['loss']=abs(e['loss']-train_edge_set[temp_key])\n",
    "                    \n",
    "#                     e['modified']=True\n",
    "#                 else:\n",
    "#                     e['modified']=False\n",
    "                loss+=e['loss']\n",
    "\n",
    "            loss=loss/event_count   \n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)   # 按照loss 值排序  或者按照edge的时间顺序排列\n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "            \n",
    " \n",
    "    return time_with_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试4-9 ~ 4-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_3=test_day_new(graph_4_3,\"graph_4_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_4=test_day_new(graph_4_4,\"graph_4_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_5=test_day_new(graph_4_5,\"graph_4_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_9=test_day_new(graph_4_9,\"graph_4_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_12=test_day_new(graph_4_12,\"graph_4_12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算IDF函数 异常分数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()#至少有两个 因此要减去2\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1\n",
    "#         add=True\n",
    "#         for line in f:\n",
    "            \n",
    "#             if find_str in line:\n",
    "# #                 print(line)\n",
    "#                 if add:\n",
    "#                     include_count+=1\n",
    "#                     add=False\n",
    "#                 l=line.strip()\n",
    "#                 jdata=eval(l)\n",
    "#                 different_neighbor.add(jdata['srcmsg'])\n",
    "#                 different_neighbor.add(jdata['dstmsg'])\n",
    "                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_IDF_by_file_in_mem(find_str,file_list):\n",
    "\n",
    "    include_count=0\n",
    "    different_neighbor=set()#至少有两个 因此要减去2\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1\n",
    "#         add=True\n",
    "#         for line in f:\n",
    "            \n",
    "#             if find_str in line:\n",
    "# #                 print(line)\n",
    "#                 if add:\n",
    "#                     include_count+=1\n",
    "#                     add=False\n",
    "#                 l=line.strip()\n",
    "#                 jdata=eval(l)\n",
    "#                 different_neighbor.add(jdata['srcmsg'])\n",
    "#                 different_neighbor.add(jdata['dstmsg'])\n",
    "                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()#至少有两个 因此要减去2\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+1*loss_std\n",
    "#     thr=0 # 因为训练的时候采用的就是整个batch均值的方式进行训练\n",
    "    print(\"thr:\",thr)\n",
    "  \n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "          \n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/(count+0.00000000001),node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以15min为间隔计算IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用之前的IDF计算方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 窗口关系建立函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有两个窗口建立函数，一个用于4-10号 一个用于4-12  有一点点小的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         'netflow',        \n",
    "        '/dev/pts',\n",
    "        'salt-minion.log',\n",
    "        'null',\n",
    "        'usr',\n",
    "         'proc',\n",
    "        'firefox',\n",
    "        'tmp',\n",
    "        'thunderbird',\n",
    "        'bin/',\n",
    "        '/data/replay_logdb',\n",
    "        '/stat',\n",
    "        '/boot',\n",
    "        'qt-opensource-linux-x64',\n",
    "        '/eraseme'\n",
    "        '675',\n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def cal_set_rel(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "       if is_include_key_word(i) is not True:\n",
    "        \n",
    "#         'netflow' not in i\n",
    "#         and 'usr' not in i and 'var' not in i\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "            if i in node_IDF_3.keys():\n",
    "                IDF3=node_IDF_3[i]\n",
    "            else:\n",
    "                IDF3=math.log((files_count_4_3)/(1))    \n",
    "            \n",
    "#             print(IDF)\n",
    "            if (IDF+IDF3)>5 :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_include_key_word_bak(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "        'null',\n",
    "        '/dev/pts',\n",
    "        'salt-minion.log',\n",
    "        '675',\n",
    "        'usr',\n",
    "         'proc',\n",
    "        '/.cache/mozilla/',\n",
    "        'tmp',\n",
    "        'thunderbird',\n",
    "        '/bin/',\n",
    "        '/sbin/sysctl',\n",
    "#         '/data/replay_logdb/',\n",
    "        '/home/admin/eraseme',\n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def cal_set_rel_bak(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if is_include_key_word_bak(i) is not True:\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))         \n",
    "\n",
    "            if (IDF)>math.log(len(file_list)*0.9/(1))  :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10_without_neg_edge\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10_without_neg_edge/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11_without_neg_edge\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11_without_neg_edge/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_12_without_neg_edge\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_12_without_neg_edge/\"+f]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    \n",
    "'graph_4_10_without_neg_edge/2018-04-10 13:31:14.548738409~2018-04-10 13:46:36.161065223.txt', \n",
    " 'graph_4_10_without_neg_edge/2018-04-10 14:02:17.001271389~2018-04-10 14:17:34.001373488.txt', \n",
    " 'graph_4_10_without_neg_edge/2018-04-10 14:17:34.001373488~2018-04-10 14:33:18.350772859.txt', \n",
    " 'graph_4_10_without_neg_edge/2018-04-10 14:33:18.350772859~2018-04-10 14:48:47.320442910.txt',\n",
    "'graph_4_10_without_neg_edge/2018-04-10 14:48:47.320442910~2018-04-10 15:03:54.307022037.txt', \n",
    " \n",
    " 'graph_4_12_without_neg_edge/2018-04-12 12:39:06.592749611~2018-04-12 12:54:44.001952179.txt', \n",
    " 'graph_4_12_without_neg_edge/2018-04-12 12:54:44.001952179~2018-04-12 13:09:55.027159876.txt', \n",
    " 'graph_4_12_without_neg_edge/2018-04-12 13:09:55.027159876~2018-04-12 13:25:06.588443090.txt', \n",
    " 'graph_4_12_without_neg_edge/2018-04-12 13:25:06.588443090~2018-04-12 13:40:07.178285911.txt'\n",
    "]\n",
    "\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-10异常检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF_3=torch.load(\"node_IDF_4_3\")\n",
    "node_IDF=torch.load(\"node_IDF_4_3\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_10_without_neg_edge/\"\n",
    "file_l=os.listdir(\"graph_4_10_without_neg_edge/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "    \n",
    "    \n",
    "# node_IDF_410=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF=torch.load(\"node_IDF_4_12\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_10_without_neg_edge/\"\n",
    "file_l=os.listdir(\"graph_4_10_without_neg_edge/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "# file_path=\"graph_4_12/\"\n",
    "# file_l=os.listdir(\"graph_4_12/\")\n",
    "# for i in file_l:\n",
    "#     file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_label={}\n",
    "\n",
    "# files = os.listdir(\"graph_4_9\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_9/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_10_without_neg_edge\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_10_without_neg_edge/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_11_without_neg_edge\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_11_without_neg_edge/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_12_without_neg_edge\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_12_without_neg_edge/\"+f]=0\n",
    "\n",
    "\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "\n",
    "    if loss_count>10:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-12异常检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF_4_12\") # 单独计算下4-12的idf值\n",
    "node_IDF_3=torch.load(\"node_IDF_4_3\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_12_without_neg_edge/\"\n",
    "file_l=os.listdir(\"graph_4_12_without_neg_edge/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "#以下变量名对检测结果影响不大，所以当初就没改   \n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_12_without_neg_edge/\"\n",
    "file_l=os.listdir(\"graph_4_12_without_neg_edge/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_12_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],file_l)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>100:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计该场景的attack edges数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            '/home/admin/clean',\n",
    "            '/dev/glx_alsa_675',\n",
    "            '/home/admin/profile',\n",
    "#             '/var/log/mail',  # 添加以后会将端口扫描的日志加入，这部分的日志会十分庞大。\n",
    "            '/tmp/memtrace.so',\n",
    "            '/var/log/xdev',\n",
    "             '/var/log/wdev',\n",
    "            'gtcache',\n",
    "#             'firefox',\n",
    "        '161.116.88.72',\n",
    "        '146.153.68.151',\n",
    "        '145.199.103.57',\n",
    "        '61.130.69.232',\n",
    "        '5.214.163.155',\n",
    "        '104.228.117.212',\n",
    "        '141.43.176.203',\n",
    "        '7.149.198.40',\n",
    "        '5.214.163.155',\n",
    "        '149.52.198.23',\n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[]\n",
    "temp_file=[\n",
    "         '2018-04-10 13:33:14.002087332~2018-04-10 13:48:35.001709448.txt',\n",
    " '2018-04-10 14:19:04.002022895~2018-04-10 14:34:39.687914951.txt',\n",
    " '2018-04-10 14:34:39.687914951~2018-04-10 14:49:52.239141721.txt',\n",
    " '2018-04-10 14:49:52.239141721~2018-04-10 15:05:26.721606494.txt',\n",
    "]\n",
    "for f in temp_file:\n",
    "    files.append(\"./graph_4_10_bak/graph_4_10/\"+f)\n",
    "    \n",
    "    \n",
    "temp_file=[\n",
    " '2018-04-12 12:37:14.001187512~2018-04-12 12:52:38.001955351.txt', \n",
    " '2018-04-12 12:52:38.001955351~2018-04-12 13:07:55.787879623.txt', \n",
    " '2018-04-12 13:07:55.787879623~2018-04-12 13:23:35.144633810.txt',\n",
    " '2018-04-12 13:23:35.144633810~2018-04-12 13:38:35.600938143.txt',\n",
    "]    \n",
    "for f in temp_file:\n",
    "    files.append(\"./graph_4_12_bak/graph_4_12/\"+f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '/run/shm/':'/run/shm/*',\n",
    "#     '/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/':'/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/*',\n",
    "   '/home/admin/.cache/mozilla/firefox/':'/home/admin/.cache/mozilla/firefox/*',\n",
    "    '/home/admin/.mozilla/firefox':'/home/admin/.mozilla/firefox*',    \n",
    "    '/data/replay_logdb/':'/data/replay_logdb/*', \n",
    "    '/home/admin/.local/share/applications/':'/home/admin/.local/share/applications/*', \n",
    "    '/usr/share/applications/':'/usr/share/applications/*', \n",
    "    '/lib/x86_64-linux-gnu/':'/lib/x86_64-linux-gnu/*',     \n",
    "    '/proc/':'/proc/*', \n",
    "     '/stat':'*/stat', \n",
    "    '/etc/bash_completion.d/':'/etc/bash_completion.d/*', \n",
    "    '/usr/bin/python2.7':'/usr/bin/python2.7/*', \n",
    "     '/usr/lib/python2.7':'/usr/lib/python2.7/*', \n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic_draw={\n",
    "    '/run/shm/':'/run/shm/*',\n",
    "   '/home/admin/.cache/mozilla/firefox/':'/home/admin/.cache/mozilla/firefox/*',\n",
    "    '/home/admin/.mozilla/firefox':'/home/admin/.mozilla/firefox*',    \n",
    "    '/data/replay_logdb/':'/data/replay_logdb/*', \n",
    "    '/home/admin/.local/share/applications/':'/home/admin/.local/share/applications/*', \n",
    "    \n",
    "    '/usr/share/applications/':'/usr/share/applications/*', \n",
    "    '/lib/x86_64-linux-gnu/':'/lib/x86_64-linux-gnu/*',     \n",
    "    '/proc/':'/proc/*', \n",
    "     '/stat':'*/stat', \n",
    "    '/etc/bash_completion.d/':'/etc/bash_completion.d/*', \n",
    "    '/usr/bin/python2.7':'/usr/bin/python2.7/*', \n",
    "     '/usr/lib/python2.7':'/usr/lib/python2.7/*', \n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "def replace_path_name_draw(path_name):\n",
    "    for i in replace_dic_draw:\n",
    "        if i in path_name:\n",
    "            return replace_dic_draw[i]\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_edge_flag(msg):\n",
    "    attack_edge_type=[\n",
    "        '/home/admin/clean',\n",
    "        '/dev/glx_alsa_675',\n",
    "        '/home/admin/profile',\n",
    "          '/var/log/xdev',\n",
    "    '/etc/passwd',\n",
    "    '161.116.88.72',\n",
    "    '146.153.68.151',\n",
    "        '/var/log/mail',\n",
    "        '/tmp/memtrace.so',\n",
    "#         '/tmp',\n",
    "        '/var/log/xdev',\n",
    "         '/var/log/wdev',\n",
    "        'gtcache',\n",
    "        'firefox',\n",
    "#         '/var/log',\n",
    "    ]\n",
    "    flag=False\n",
    "    for i in attack_edge_type:\n",
    "        if i in msg:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg20]",
   "language": "python",
   "name": "conda-env-pyg20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
