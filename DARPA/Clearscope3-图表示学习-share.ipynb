{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成每天的测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算IDF字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的代码使用jinyuan 优化过后的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 4-7的IDF值\n",
    "\n",
    "# share_node_IDF = mp.Manager().dict()\n",
    "\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                node_set.add(str(jdata['srcmsg']))\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                node_set.add(str(jdata['dstmsg'])) \n",
    "\n",
    "\n",
    "node_list=list(node_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mem_list=[]\n",
    "for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        files_mem_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF={}\n",
    "for n in tqdm(node_list):  \n",
    "    find_str=list(eval(n).values())[0]\n",
    "    IDF=cal_IDF_by_file_in_mem(n,files_mem_list)\n",
    "    node_IDF[n]=IDF\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_4_3-7\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 窗口关系建立函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF_4_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "def cal_set_rel(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if 'netflow' not in i and 'glx_alsa_675' not in i and '/data/system/' not in i and '/storage/emulated/' not in i and  '/data/data/com.android' not in i and  '/proc/' not in i and 'nz9885vc.default' not in i :\n",
    "\n",
    "#         'netflow' not in i\n",
    "#         and 'usr' not in i and 'var' not in i\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "            if i in node_IDF_4_4_7.keys():\n",
    "                IDF4=node_IDF_4_4_7[i]\n",
    "            else:\n",
    "                IDF4=math.log(len(file_list_4_4_7)/(1))    \n",
    "            \n",
    "#             print(IDF)\n",
    "            if (IDF+IDF4)>9:\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测4-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_10\")\n",
    "node_IDF=torch.load(\"node_IDF_4_4-7\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_4_10=[]\n",
    "\n",
    "\n",
    "file_l=os.listdir(\"graph_4_10\")\n",
    "index_count=0\n",
    "for f_path in (file_l):\n",
    "    f=open(\"graph_4_10/\"+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],file_list_4_4_7)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_10.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>100:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测4-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF_4_11\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "y_data_4_11=[]\n",
    "df_list_4_11=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_4_11=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_11.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_11/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],file_list_4_4_7)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_11.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "    \n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计attack edge数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            'shared_files',\n",
    "        'csb.tracee.27331.27355',\n",
    "        'netrecon',\n",
    "#         '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "     \n",
    "#             'firefox',\n",
    "        '153.178.46.202',\n",
    "       '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        '140.57.183.17',\n",
    "      \n",
    "        \n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[\n",
    "    \n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    " '/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "     '/data/data/org.mozilla.fennec_firefox_dev/files/':'/data/data/org.mozilla.fennec_firefox_dev/files/*',\n",
    "    '/system/fonts/':'/system/fonts/*',\n",
    "    '/data/data/com.android.email/cache/':'/data/data/com.android.email/cache/*',\n",
    "    '/data/data/com.android.email/files/':'/data/data/com.android.email/files/*',\n",
    "    'UNNAMED':'UNNAMED:*',\n",
    "    \n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                #不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#                                 不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_edge_flag(msg):\n",
    "    attack_edge_type=[\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/shared_files',\n",
    "        '/data/local/tmp',\n",
    "        'csb.tracee.27331.27355',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/csb.tracee.27331.27355',\n",
    "        '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        'glx_alsa_675',\n",
    "  \n",
    "    ]\n",
    "    flag=False\n",
    "    for i in attack_edge_type:\n",
    "        if i in msg:\n",
    "            flag=True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测结果统计分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿用其他数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg20]",
   "language": "python",
   "name": "conda-env-pyg20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
