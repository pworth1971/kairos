{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# msg structure:      [src_node_feature,edge_attr,dst_node_feature]\n",
    "\n",
    "# compute the best partition\n",
    "import datetime\n",
    "# import community as community_louvain\n",
    "\n",
    "import xxhash\n",
    "# Find the edge index which the edge vector is corresponding to\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    loss=[]\n",
    "    for i in link_pred_ratio:\n",
    "        loss.append(criterion(i,torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[] \n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_autoencoder(decoded,msg):\n",
    "    loss=[] \n",
    "    for i in range(len(decoded)):\n",
    "        loss.append(criterion(decoded[i].reshape(1,-1),msg[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_clearscope3_dataset_db',\n",
    "                           host = '/var/run/postgresql/',\n",
    "                           user = 'postgres',\n",
    "                           password = 'postgres',\n",
    "                           port = '5432'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_4=torch.load(\"./train_graphs/graph_4_4.TemporalData.simple\").to(device=device)\n",
    "graph_4_5=torch.load(\"./train_graphs/graph_4_5.TemporalData.simple\").to(device=device)\n",
    "graph_4_6=torch.load(\"./train_graphs/graph_4_6.TemporalData.simple\").to(device=device)\n",
    "\n",
    "\n",
    "train_data=graph_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # nodeid => msg and node hash => nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 1,\n",
    " 2: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 2,\n",
    " 3: 'EVENT_READ',\n",
    " 'EVENT_READ': 3,\n",
    " 4: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "# max_node_num = max(torch.cat([data.dst,data.src]))+1\n",
    "# max_node_num = data.num_nodes+1\n",
    "max_node_num = 172724  # +1\n",
    "# min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(out_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        \n",
    "        self.lin_seq = nn.Sequential(\n",
    "            \n",
    "            Linear(in_channels*4, in_channels*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*8, in_channels*2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*2, int(in_channels//2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels//2), train_data.msg.shape[1]-32)                   \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "         \n",
    "        h = self.lin_seq (h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "memory_dim = 100         # node state\n",
    "time_dim = 100\n",
    "embedding_dim = 100      # edge embedding\n",
    "\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "saved_nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1024\n",
    "def train(train_data):\n",
    "\n",
    "    \n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "#     print(\"train_before_stage_data:\",train_data)\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "        \n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "#         n_id = torch.cat([src, pos_dst, neg_src, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "      \n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        \n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        \n",
    "#         y_true = torch.cat([torch.zeros(pos_out.size(0),1),torch.ones(neg_out.size(0),1)], dim=0)#\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "          \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "#         loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "#         loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "#         for i in range(len(src)):\n",
    "#             saved_nodes.add(int(src[i]))\n",
    "#             saved_nodes.add(int(pos_dst[i]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "#         print(z.shape)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "#     print(\"trained_stage_data:\",train_data)\n",
    "    return total_loss / train_data.num_events\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                   | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.3191\n",
      "  Epoch: 01, Loss: 1.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███                                                                                        | 1/30 [00:55<26:41, 55.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 1.0991\n",
      "  Epoch: 02, Loss: 0.8855\n",
      "  Epoch: 02, Loss: 0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██████                                                                                     | 2/30 [01:49<25:24, 54.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 02, Loss: 1.0007\n",
      "  Epoch: 03, Loss: 0.8215\n",
      "  Epoch: 03, Loss: 0.8030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█████████                                                                                  | 3/30 [02:43<24:23, 54.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 03, Loss: 0.9522\n",
      "  Epoch: 04, Loss: 0.7876\n",
      "  Epoch: 04, Loss: 0.7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|████████████▏                                                                              | 4/30 [03:36<23:26, 54.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 04, Loss: 0.9198\n",
      "  Epoch: 05, Loss: 0.7656\n",
      "  Epoch: 05, Loss: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████▏                                                                           | 5/30 [04:30<22:31, 54.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 05, Loss: 0.9010\n",
      "  Epoch: 06, Loss: 0.7479\n",
      "  Epoch: 06, Loss: 0.6972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████▏                                                                        | 6/30 [05:25<21:37, 54.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 06, Loss: 0.8806\n",
      "  Epoch: 07, Loss: 0.7362\n",
      "  Epoch: 07, Loss: 0.6837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████████████▏                                                                     | 7/30 [06:19<20:43, 54.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 07, Loss: 0.8706\n",
      "  Epoch: 08, Loss: 0.7250\n",
      "  Epoch: 08, Loss: 0.6708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████████████████▎                                                                  | 8/30 [07:13<19:50, 54.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 08, Loss: 0.8649\n",
      "  Epoch: 09, Loss: 0.7208\n",
      "  Epoch: 09, Loss: 0.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████▎                                                               | 9/30 [08:07<18:57, 54.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 09, Loss: 0.8625\n",
      "  Epoch: 10, Loss: 0.7164\n",
      "  Epoch: 10, Loss: 0.6570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████                                                            | 10/30 [09:01<18:03, 54.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 10, Loss: 0.8285\n",
      "  Epoch: 11, Loss: 0.7078\n",
      "  Epoch: 11, Loss: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████████████████████                                                         | 11/30 [09:56<17:10, 54.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 11, Loss: 0.8179\n",
      "  Epoch: 12, Loss: 0.7044\n",
      "  Epoch: 12, Loss: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████                                                      | 12/30 [10:50<16:16, 54.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 12, Loss: 0.8150\n",
      "  Epoch: 13, Loss: 0.7021\n",
      "  Epoch: 13, Loss: 0.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████████████████████████                                                   | 13/30 [11:44<15:23, 54.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 13, Loss: 0.8056\n",
      "  Epoch: 14, Loss: 0.6997\n",
      "  Epoch: 14, Loss: 0.6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████████                                                | 14/30 [12:39<14:28, 54.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 14, Loss: 0.8019\n",
      "  Epoch: 15, Loss: 0.6974\n",
      "  Epoch: 15, Loss: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████████                                             | 15/30 [13:33<13:34, 54.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 15, Loss: 0.8006\n",
      "  Epoch: 16, Loss: 0.6951\n",
      "  Epoch: 16, Loss: 0.6327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████████████████████████████                                          | 16/30 [14:27<12:40, 54.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 16, Loss: 0.7980\n",
      "  Epoch: 17, Loss: 0.6941\n",
      "  Epoch: 17, Loss: 0.6295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████████████████████████████████                                       | 17/30 [15:22<11:45, 54.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 17, Loss: 0.7960\n",
      "  Epoch: 18, Loss: 0.6919\n",
      "  Epoch: 18, Loss: 0.6281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████                                    | 18/30 [16:16<10:51, 54.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 18, Loss: 0.7945\n",
      "  Epoch: 19, Loss: 0.6920\n",
      "  Epoch: 19, Loss: 0.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████                                 | 19/30 [17:10<09:57, 54.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 19, Loss: 0.7932\n",
      "  Epoch: 20, Loss: 0.6910\n",
      "  Epoch: 20, Loss: 0.6288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████████                              | 20/30 [18:05<09:03, 54.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 20, Loss: 0.7912\n",
      "  Epoch: 21, Loss: 0.6891\n",
      "  Epoch: 21, Loss: 0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████▉                           | 21/30 [18:59<08:09, 54.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 21, Loss: 0.7901\n",
      "  Epoch: 22, Loss: 0.6902\n",
      "  Epoch: 22, Loss: 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████████████████████████████████████████                        | 22/30 [19:54<07:15, 54.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 22, Loss: 0.7871\n",
      "  Epoch: 23, Loss: 0.6891\n",
      "  Epoch: 23, Loss: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████                     | 23/30 [20:48<06:21, 54.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 23, Loss: 0.7897\n",
      "  Epoch: 24, Loss: 0.6880\n",
      "  Epoch: 24, Loss: 0.6260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████                  | 24/30 [21:43<05:26, 54.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 24, Loss: 0.7849\n",
      "  Epoch: 25, Loss: 0.6886\n",
      "  Epoch: 25, Loss: 0.6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████               | 25/30 [22:37<04:32, 54.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 25, Loss: 0.7879\n",
      "  Epoch: 26, Loss: 0.6881\n",
      "  Epoch: 26, Loss: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|██████████████████████████████████████████████████████████████████████████████            | 26/30 [23:31<03:37, 54.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 26, Loss: 0.7872\n",
      "  Epoch: 27, Loss: 0.6885\n",
      "  Epoch: 27, Loss: 0.6222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████████████         | 27/30 [24:26<02:43, 54.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 27, Loss: 0.7841\n",
      "  Epoch: 28, Loss: 0.6863\n",
      "  Epoch: 28, Loss: 0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████████████████      | 28/30 [25:20<01:48, 54.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 28, Loss: 0.7858\n",
      "  Epoch: 29, Loss: 0.6859\n",
      "  Epoch: 29, Loss: 0.6219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████   | 29/30 [26:15<00:54, 54.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 29, Loss: 0.7808\n",
      "  Epoch: 30, Loss: 0.6869\n",
      "  Epoch: 30, Loss: 0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 30/30 [27:09<00:00, 54.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 30, Loss: 0.7835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_graphs=[graph_4_4, graph_4_5, graph_4_6]\n",
    "\n",
    "for epoch in tqdm(range(1, 31)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "#     scheduler.step()\n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "os.system(\"mkdir -p ./models/\")\n",
    "torch.save(model,\"./models/model_saved_share.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "@torch.no_grad()\n",
    "def test_day_new(inference_data,path):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  # Start with a fresh memory.  \n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    \n",
    "    time_with_loss={}\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([]).to(device=device)\n",
    "    total_edges=0\n",
    "\n",
    "\n",
    "    start_time=inference_data.t[0]\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    \n",
    "    loss_list=[]\n",
    "    \n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "    # Record the running time to evaluate the performance\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        \n",
    "       \n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        \n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "#         y_true = torch.cat(\n",
    "#             [torch.ones(pos_out.size(0))], dim=0).to(torch.long)     \n",
    "#         y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        # Only consider which edge hasn't been correctly predicted.\n",
    "        # For benign graphs, the behaviors patterns are similar and therefore their losses are small\n",
    "        # For anoamlous behaviors, some behaviors might not be seen before, so the probability of predicting those edges are low. Thus their losses are high.\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "     \n",
    "        \n",
    "        # update the edges in the batch to the memory and neighbor_loader\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        # compute the loss for each edge\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode=int(src[i])\n",
    "            dstnode=int(pos_dst[i])  \n",
    "            \n",
    "            srcmsg=str(nodeid2msg[srcnode]) \n",
    "            dstmsg=str(nodeid2msg[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1)   \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "            \n",
    "#             if \"netflow\" in srcmsg or \"netflow\" in dstmsg:\n",
    "#                 temp_dic['loss']=0\n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000000000*15:\n",
    "            # Here is a checkpoint, which records all edge losses in the current time window\n",
    "#             loss=total_loss/event_count\n",
    "            time_interval=ns_time_to_datetime_US(start_time)+\"~\"+ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                \n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "            \n",
    "            for e in edge_list: \n",
    "#                 temp_key=e['srcmsg']+e['dstmsg']+e['edge_type']\n",
    "#                 if temp_key in train_edge_set:      \n",
    "# #                     e['loss']=(e['loss']-train_edge_set[temp_key]) if e['loss']>=train_edge_set[temp_key] else 0  \n",
    "# #                     e['loss']=abs(e['loss']-train_edge_set[temp_key])\n",
    "                    \n",
    "#                     e['modified']=True\n",
    "#                 else:\n",
    "#                     e['modified']=False\n",
    "                loss+=e['loss']\n",
    "\n",
    "            loss=loss/event_count   \n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)  # Rank the results based on edge losses\n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "            \n",
    " \n",
    "    return time_with_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_7=torch.load(\"./train_graphs/graph_4_7.TemporalData.simple\").to(device=device)\n",
    "graph_4_10=torch.load(\"./train_graphs/graph_4_10.TemporalData.simple\").to(device=device)\n",
    "graph_4_11=torch.load(\"./train_graphs/graph_4_11.TemporalData.simple\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./model_saved_share.pt\", map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1357851], msg=[1357851, 40], src=[1357851], t=[1357851])\n",
      "Time: 2018-04-04 00:00:00.030000000~2018-04-04 00:18:00.409000000, Loss: 2.9072, Nodes_count: 45, Cost Time: 0.09s\n",
      "Time: 2018-04-04 00:18:00.409000000~2018-04-04 00:44:00.541000000, Loss: 3.5271, Nodes_count: 81, Cost Time: 0.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinyuanl/anaconda3/envs/kairos/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py:211: UserWarning: operator() profile_node %28 : int[] = prim::profile_ivalue(%size.4)\n",
      " does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)\n",
      "  alpha = softmax(alpha, index, ptr, size_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2018-04-04 00:44:00.541000000~2018-04-04 01:01:40.901000000, Loss: 3.2534, Nodes_count: 103, Cost Time: 0.22s\n",
      "Time: 2018-04-04 01:01:40.901000000~2018-04-04 01:28:00.498000000, Loss: 3.3977, Nodes_count: 129, Cost Time: 0.28s\n",
      "Time: 2018-04-04 01:28:00.498000000~2018-04-04 01:46:29.679000000, Loss: 3.1665, Nodes_count: 145, Cost Time: 0.34s\n",
      "Time: 2018-04-04 01:46:29.679000000~2018-04-04 02:13:00.519000000, Loss: 3.5923, Nodes_count: 160, Cost Time: 0.40s\n",
      "Time: 2018-04-04 02:13:00.519000000~2018-04-04 02:30:02.355000000, Loss: 3.2887, Nodes_count: 195, Cost Time: 0.46s\n",
      "Time: 2018-04-04 02:30:02.355000000~2018-04-04 02:57:00.339000000, Loss: 3.4362, Nodes_count: 215, Cost Time: 0.53s\n",
      "Time: 2018-04-04 02:57:00.339000000~2018-04-04 03:19:23.600000000, Loss: 3.2748, Nodes_count: 233, Cost Time: 0.59s\n",
      "Time: 2018-04-04 03:19:23.600000000~2018-04-04 03:41:40.895000000, Loss: 3.3659, Nodes_count: 250, Cost Time: 0.65s\n",
      "Time: 2018-04-04 03:41:40.895000000~2018-04-04 04:02:12.886000000, Loss: 3.3543, Nodes_count: 281, Cost Time: 0.71s\n",
      "Time: 2018-04-04 04:02:12.886000000~2018-04-04 04:28:26.253000000, Loss: 3.5015, Nodes_count: 304, Cost Time: 0.77s\n",
      "Time: 2018-04-04 04:28:26.253000000~2018-04-04 04:47:03.432000000, Loss: 3.1277, Nodes_count: 325, Cost Time: 0.84s\n",
      "Time: 2018-04-04 04:47:03.432000000~2018-04-04 05:13:47.869000000, Loss: 3.4695, Nodes_count: 346, Cost Time: 0.90s\n",
      "Time: 2018-04-04 05:13:47.869000000~2018-04-04 05:35:28.922000000, Loss: 3.2991, Nodes_count: 365, Cost Time: 0.96s\n",
      "Time: 2018-04-04 05:35:28.922000000~2018-04-04 05:57:55.470000000, Loss: 3.3304, Nodes_count: 390, Cost Time: 1.02s\n",
      "Time: 2018-04-04 05:57:55.470000000~2018-04-04 06:20:00.400000000, Loss: 3.3199, Nodes_count: 408, Cost Time: 1.09s\n",
      "Time: 2018-04-04 06:20:00.400000000~2018-04-04 06:42:05.998000000, Loss: 3.4310, Nodes_count: 425, Cost Time: 1.15s\n",
      "Time: 2018-04-04 06:42:05.998000000~2018-04-04 07:00:01.112000000, Loss: 3.3569, Nodes_count: 456, Cost Time: 1.21s\n",
      "Time: 2018-04-04 07:00:01.112000000~2018-04-04 07:25:31.328000000, Loss: 3.3997, Nodes_count: 476, Cost Time: 1.28s\n",
      "Time: 2018-04-04 07:25:31.328000000~2018-04-04 07:45:00.920000000, Loss: 3.0506, Nodes_count: 499, Cost Time: 1.34s\n",
      "Time: 2018-04-04 07:45:00.920000000~2018-04-04 08:05:00.337000000, Loss: 3.1763, Nodes_count: 521, Cost Time: 1.40s\n",
      "Time: 2018-04-04 08:05:00.337000000~2018-04-04 08:30:00.340000000, Loss: 3.4854, Nodes_count: 549, Cost Time: 1.46s\n",
      "Time: 2018-04-04 08:30:00.340000000~2018-04-04 08:53:44.098000000, Loss: 3.2650, Nodes_count: 566, Cost Time: 1.52s\n",
      "Time: 2018-04-04 08:53:44.098000000~2018-04-04 09:08:54.146000000, Loss: 1.2779, Nodes_count: 682, Cost Time: 3.42s\n",
      "Time: 2018-04-04 09:08:54.146000000~2018-04-04 09:24:14.060000000, Loss: 2.8014, Nodes_count: 742, Cost Time: 3.80s\n",
      "Time: 2018-04-04 09:24:14.060000000~2018-04-04 09:46:59.997000000, Loss: 1.3097, Nodes_count: 784, Cost Time: 4.99s\n",
      "Time: 2018-04-04 09:46:59.997000000~2018-04-04 10:07:04.753000000, Loss: 2.6648, Nodes_count: 1076, Cost Time: 5.79s\n",
      "Time: 2018-04-04 10:07:04.753000000~2018-04-04 10:24:56.390000000, Loss: 1.7221, Nodes_count: 1202, Cost Time: 7.63s\n",
      "Time: 2018-04-04 10:24:56.390000000~2018-04-04 10:43:20.118000000, Loss: 1.2907, Nodes_count: 1272, Cost Time: 9.86s\n",
      "Time: 2018-04-04 10:43:20.118000000~2018-04-04 11:00:02.549000000, Loss: 1.2865, Nodes_count: 1476, Cost Time: 14.91s\n",
      "Time: 2018-04-04 11:00:02.549000000~2018-04-04 11:15:17.515000000, Loss: 2.0062, Nodes_count: 1551, Cost Time: 16.83s\n",
      "Time: 2018-04-04 11:15:17.515000000~2018-04-04 11:31:03.856000000, Loss: 2.7898, Nodes_count: 1667, Cost Time: 17.65s\n",
      "Time: 2018-04-04 11:31:03.856000000~2018-04-04 11:46:32.145000000, Loss: 2.6718, Nodes_count: 2097, Cost Time: 19.10s\n",
      "Time: 2018-04-04 11:46:32.145000000~2018-04-04 12:01:38.539000000, Loss: 1.2445, Nodes_count: 2208, Cost Time: 22.73s\n",
      "Time: 2018-04-04 12:01:38.539000000~2018-04-04 12:16:41.081000000, Loss: 2.2835, Nodes_count: 3487, Cost Time: 25.66s\n",
      "Time: 2018-04-04 12:16:41.081000000~2018-04-04 12:34:19.710000000, Loss: 2.6053, Nodes_count: 4046, Cost Time: 27.39s\n",
      "Time: 2018-04-04 12:34:19.710000000~2018-04-04 12:49:22.540000000, Loss: 2.6948, Nodes_count: 4993, Cost Time: 29.40s\n",
      "Time: 2018-04-04 12:49:22.540000000~2018-04-04 13:08:07.223000000, Loss: 2.5387, Nodes_count: 5326, Cost Time: 30.79s\n",
      "Time: 2018-04-04 13:08:07.223000000~2018-04-04 13:30:01.377000000, Loss: 2.6829, Nodes_count: 5386, Cost Time: 31.21s\n",
      "Time: 2018-04-04 13:30:01.377000000~2018-04-04 13:45:50.641000000, Loss: 2.7361, Nodes_count: 5543, Cost Time: 32.30s\n",
      "Time: 2018-04-04 13:45:50.641000000~2018-04-04 14:01:08.905000000, Loss: 2.6840, Nodes_count: 5895, Cost Time: 33.83s\n",
      "Time: 2018-04-04 14:01:08.905000000~2018-04-04 14:17:40.088000000, Loss: 1.1999, Nodes_count: 5944, Cost Time: 36.63s\n",
      "Time: 2018-04-04 14:17:40.088000000~2018-04-04 14:37:36.937000000, Loss: 2.1764, Nodes_count: 5980, Cost Time: 37.62s\n",
      "Time: 2018-04-04 14:37:36.937000000~2018-04-04 14:56:53.830000000, Loss: 2.0719, Nodes_count: 6235, Cost Time: 40.29s\n",
      "Time: 2018-04-04 14:56:53.830000000~2018-04-04 15:11:59.827000000, Loss: 1.3444, Nodes_count: 6276, Cost Time: 41.88s\n",
      "Time: 2018-04-04 15:11:59.827000000~2018-04-04 15:30:59.833000000, Loss: 2.0094, Nodes_count: 6503, Cost Time: 44.31s\n",
      "Time: 2018-04-04 15:30:59.833000000~2018-04-04 15:46:27.669000000, Loss: 2.4621, Nodes_count: 6619, Cost Time: 45.34s\n",
      "Time: 2018-04-04 15:46:27.669000000~2018-04-04 16:01:28.278000000, Loss: 1.8612, Nodes_count: 6824, Cost Time: 64.59s\n",
      "Time: 2018-04-04 16:01:28.278000000~2018-04-04 16:19:10.228000000, Loss: 1.7707, Nodes_count: 6953, Cost Time: 71.24s\n",
      "Time: 2018-04-04 16:19:10.228000000~2018-04-04 16:34:10.523000000, Loss: 2.5449, Nodes_count: 6992, Cost Time: 71.92s\n",
      "Time: 2018-04-04 16:34:10.523000000~2018-04-04 16:49:13.914000000, Loss: 1.7587, Nodes_count: 7254, Cost Time: 75.92s\n",
      "Time: 2018-04-04 16:49:13.914000000~2018-04-04 17:04:39.724000000, Loss: 2.5020, Nodes_count: 7346, Cost Time: 76.97s\n",
      "Time: 2018-04-04 17:04:39.724000000~2018-04-04 17:28:27.768000000, Loss: 2.5792, Nodes_count: 7375, Cost Time: 77.12s\n",
      "Time: 2018-04-04 17:28:27.768000000~2018-04-04 17:50:34.864000000, Loss: 3.2334, Nodes_count: 7388, Cost Time: 77.19s\n",
      "Time: 2018-04-04 17:50:34.864000000~2018-04-04 18:12:26.456000000, Loss: 3.0978, Nodes_count: 7404, Cost Time: 77.25s\n",
      "Time: 2018-04-04 18:12:26.456000000~2018-04-04 18:30:37.518000000, Loss: 3.3270, Nodes_count: 7430, Cost Time: 77.31s\n",
      "Time: 2018-04-04 18:30:37.518000000~2018-04-04 18:55:41.922000000, Loss: 3.4165, Nodes_count: 7446, Cost Time: 77.38s\n",
      "Time: 2018-04-04 18:55:41.922000000~2018-04-04 19:15:01.002000000, Loss: 3.1138, Nodes_count: 7468, Cost Time: 77.44s\n",
      "Time: 2018-04-04 19:15:01.002000000~2018-04-04 19:34:59.220000000, Loss: 3.3045, Nodes_count: 7484, Cost Time: 77.50s\n",
      "Time: 2018-04-04 19:34:59.220000000~2018-04-04 20:00:00.180000000, Loss: 3.3473, Nodes_count: 7510, Cost Time: 77.57s\n",
      "Time: 2018-04-04 20:00:00.180000000~2018-04-04 20:21:35.285000000, Loss: 2.8183, Nodes_count: 7521, Cost Time: 77.63s\n",
      "Time: 2018-04-04 20:21:35.285000000~2018-04-04 20:43:53.871000000, Loss: 3.2461, Nodes_count: 7545, Cost Time: 77.69s\n",
      "Time: 2018-04-04 20:43:53.871000000~2018-04-04 21:05:30.588000000, Loss: 3.3278, Nodes_count: 7568, Cost Time: 77.75s\n",
      "Time: 2018-04-04 21:05:30.588000000~2018-04-04 21:27:32.722000000, Loss: 3.3697, Nodes_count: 7594, Cost Time: 77.82s\n",
      "Time: 2018-04-04 21:27:32.722000000~2018-04-04 21:48:59.197000000, Loss: 3.3748, Nodes_count: 7610, Cost Time: 77.88s\n",
      "Time: 2018-04-04 21:48:59.197000000~2018-04-04 22:09:45.403000000, Loss: 3.3735, Nodes_count: 7628, Cost Time: 77.94s\n",
      "Time: 2018-04-04 22:09:45.403000000~2018-04-04 22:30:16.698000000, Loss: 3.3724, Nodes_count: 7655, Cost Time: 78.00s\n",
      "Time: 2018-04-04 22:30:16.698000000~2018-04-04 22:52:49.267000000, Loss: 3.1554, Nodes_count: 7669, Cost Time: 78.06s\n",
      "Time: 2018-04-04 22:52:49.267000000~2018-04-04 23:13:54.971000000, Loss: 3.3406, Nodes_count: 7695, Cost Time: 78.13s\n",
      "Time: 2018-04-04 23:13:54.971000000~2018-04-04 23:35:49.374000000, Loss: 3.3449, Nodes_count: 7712, Cost Time: 78.19s\n",
      "Time: 2018-04-04 23:35:49.374000000~2018-04-04 23:59:02.265000000, Loss: 3.4472, Nodes_count: 7740, Cost Time: 78.25s\n"
     ]
    }
   ],
   "source": [
    "ans_4_4=test_day_new(graph_4_4,\"graph_4_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[840914], msg=[840914, 40], src=[840914], t=[840914])\n",
      "Time: 2018-04-05 00:00:00.041000000~2018-04-05 00:19:19.331000000, Loss: 3.0705, Nodes_count: 42, Cost Time: 0.06s\n",
      "Time: 2018-04-05 00:19:19.331000000~2018-04-05 00:44:22.644000000, Loss: 3.5931, Nodes_count: 75, Cost Time: 0.12s\n",
      "Time: 2018-04-05 00:44:22.644000000~2018-04-05 01:05:24.697000000, Loss: 3.3852, Nodes_count: 93, Cost Time: 0.18s\n",
      "Time: 2018-04-05 01:05:24.697000000~2018-04-05 01:27:27.947000000, Loss: 3.2261, Nodes_count: 118, Cost Time: 0.25s\n",
      "Time: 2018-04-05 01:27:27.947000000~2018-04-05 01:50:05.181000000, Loss: 3.4443, Nodes_count: 138, Cost Time: 0.31s\n",
      "Time: 2018-04-05 01:50:05.181000000~2018-04-05 02:15:00.642000000, Loss: 3.5055, Nodes_count: 168, Cost Time: 0.37s\n",
      "Time: 2018-04-05 02:15:00.642000000~2018-04-05 02:37:19.553000000, Loss: 3.4018, Nodes_count: 186, Cost Time: 0.43s\n",
      "Time: 2018-04-05 02:37:19.553000000~2018-04-05 02:58:26.087000000, Loss: 3.4682, Nodes_count: 216, Cost Time: 0.50s\n",
      "Time: 2018-04-05 02:58:26.087000000~2018-04-05 03:22:10.102000000, Loss: 3.3261, Nodes_count: 231, Cost Time: 0.56s\n",
      "Time: 2018-04-05 03:22:10.102000000~2018-04-05 03:45:00.640000000, Loss: 3.4450, Nodes_count: 259, Cost Time: 0.62s\n",
      "Time: 2018-04-05 03:45:00.640000000~2018-04-05 04:05:13.730000000, Loss: 3.3324, Nodes_count: 283, Cost Time: 0.69s\n",
      "Time: 2018-04-05 04:05:13.730000000~2018-04-05 04:30:00.519000000, Loss: 3.5575, Nodes_count: 309, Cost Time: 0.75s\n",
      "Time: 2018-04-05 04:30:00.519000000~2018-04-05 04:50:19.505000000, Loss: 3.1943, Nodes_count: 323, Cost Time: 0.81s\n",
      "Time: 2018-04-05 04:50:19.505000000~2018-04-05 05:12:47.557000000, Loss: 3.1610, Nodes_count: 341, Cost Time: 0.87s\n",
      "Time: 2018-04-05 05:12:47.557000000~2018-04-05 05:34:28.245000000, Loss: 3.3612, Nodes_count: 366, Cost Time: 0.93s\n",
      "Time: 2018-04-05 05:34:28.245000000~2018-04-05 05:56:18.371000000, Loss: 3.0792, Nodes_count: 390, Cost Time: 0.99s\n",
      "Time: 2018-04-05 05:56:18.371000000~2018-04-05 06:18:38.888000000, Loss: 3.1839, Nodes_count: 409, Cost Time: 1.06s\n",
      "Time: 2018-04-05 06:18:38.888000000~2018-04-05 06:39:58.601000000, Loss: 3.0550, Nodes_count: 428, Cost Time: 1.12s\n",
      "Time: 2018-04-05 06:39:58.601000000~2018-04-05 07:01:00.582000000, Loss: 3.1367, Nodes_count: 453, Cost Time: 1.18s\n",
      "Time: 2018-04-05 07:01:00.582000000~2018-04-05 07:23:23.952000000, Loss: 3.2509, Nodes_count: 466, Cost Time: 1.24s\n",
      "Time: 2018-04-05 07:23:23.952000000~2018-04-05 07:44:31.912000000, Loss: 3.0395, Nodes_count: 491, Cost Time: 1.31s\n",
      "Time: 2018-04-05 07:44:31.912000000~2018-04-05 08:01:00.827000000, Loss: 3.2115, Nodes_count: 516, Cost Time: 1.37s\n",
      "Time: 2018-04-05 08:01:00.827000000~2018-04-05 08:27:13.911000000, Loss: 3.4067, Nodes_count: 540, Cost Time: 1.43s\n",
      "Time: 2018-04-05 08:27:13.911000000~2018-04-05 08:49:43.480000000, Loss: 3.0629, Nodes_count: 558, Cost Time: 1.50s\n",
      "Time: 2018-04-05 08:49:43.480000000~2018-04-05 09:07:37.850000000, Loss: 2.6299, Nodes_count: 640, Cost Time: 1.96s\n",
      "Time: 2018-04-05 09:07:37.850000000~2018-04-05 09:23:08.586000000, Loss: 2.8073, Nodes_count: 725, Cost Time: 2.33s\n",
      "Time: 2018-04-05 09:23:08.586000000~2018-04-05 09:38:53.652000000, Loss: 2.5304, Nodes_count: 1444, Cost Time: 4.64s\n",
      "Time: 2018-04-05 09:38:53.652000000~2018-04-05 09:53:59.568000000, Loss: 1.9553, Nodes_count: 1604, Cost Time: 6.72s\n",
      "Time: 2018-04-05 09:53:59.568000000~2018-04-05 10:09:49.956000000, Loss: 2.0829, Nodes_count: 1723, Cost Time: 7.67s\n",
      "Time: 2018-04-05 10:09:49.956000000~2018-04-05 10:25:38.986000000, Loss: 2.6596, Nodes_count: 1756, Cost Time: 7.94s\n",
      "Time: 2018-04-05 10:25:38.986000000~2018-04-05 10:41:04.361000000, Loss: 2.1419, Nodes_count: 1917, Cost Time: 9.35s\n",
      "Time: 2018-04-05 10:41:04.361000000~2018-04-05 10:57:00.432000000, Loss: 2.4928, Nodes_count: 2079, Cost Time: 10.38s\n",
      "Time: 2018-04-05 10:57:00.432000000~2018-04-05 11:13:38.196000000, Loss: 1.9055, Nodes_count: 2111, Cost Time: 11.11s\n",
      "Time: 2018-04-05 11:13:38.196000000~2018-04-05 11:28:40.147000000, Loss: 2.4849, Nodes_count: 2313, Cost Time: 12.21s\n",
      "Time: 2018-04-05 11:28:40.147000000~2018-04-05 11:43:41.792000000, Loss: 1.2132, Nodes_count: 2392, Cost Time: 14.82s\n",
      "Time: 2018-04-05 11:43:41.792000000~2018-04-05 11:59:09.930000000, Loss: 2.0177, Nodes_count: 2449, Cost Time: 15.68s\n",
      "Time: 2018-04-05 11:59:09.930000000~2018-04-05 12:14:12.827000000, Loss: 2.5918, Nodes_count: 3626, Cost Time: 19.49s\n",
      "Time: 2018-04-05 12:14:12.827000000~2018-04-05 12:32:50.607000000, Loss: 1.7852, Nodes_count: 3659, Cost Time: 22.02s\n",
      "Time: 2018-04-05 12:32:50.607000000~2018-04-05 12:56:48.191000000, Loss: 2.1852, Nodes_count: 3769, Cost Time: 22.93s\n",
      "Time: 2018-04-05 12:56:48.191000000~2018-04-05 13:14:01.066000000, Loss: 2.7621, Nodes_count: 3793, Cost Time: 23.15s\n",
      "Time: 2018-04-05 13:14:01.066000000~2018-04-05 13:29:05.129000000, Loss: 2.3913, Nodes_count: 3935, Cost Time: 24.68s\n",
      "Time: 2018-04-05 13:29:05.129000000~2018-04-05 13:45:20.418000000, Loss: 2.7226, Nodes_count: 3998, Cost Time: 25.05s\n",
      "Time: 2018-04-05 13:45:20.418000000~2018-04-05 14:00:32.997000000, Loss: 2.3770, Nodes_count: 4137, Cost Time: 26.19s\n",
      "Time: 2018-04-05 14:00:32.997000000~2018-04-05 14:20:15.390000000, Loss: 2.2119, Nodes_count: 4208, Cost Time: 26.82s\n",
      "Time: 2018-04-05 14:20:15.390000000~2018-04-05 14:36:58.088000000, Loss: 1.1131, Nodes_count: 4310, Cost Time: 31.52s\n",
      "Time: 2018-04-05 14:36:58.088000000~2018-04-05 14:51:59.209000000, Loss: 1.6614, Nodes_count: 4369, Cost Time: 33.38s\n",
      "Time: 2018-04-05 14:51:59.209000000~2018-04-05 15:06:59.385000000, Loss: 2.3050, Nodes_count: 4722, Cost Time: 34.93s\n",
      "Time: 2018-04-05 15:06:59.385000000~2018-04-05 15:22:31.261000000, Loss: 1.7152, Nodes_count: 4906, Cost Time: 37.97s\n",
      "Time: 2018-04-05 15:22:31.261000000~2018-04-05 15:37:54.348000000, Loss: 2.3982, Nodes_count: 5616, Cost Time: 40.77s\n",
      "Time: 2018-04-05 15:37:54.348000000~2018-04-05 15:53:34.793000000, Loss: 2.6217, Nodes_count: 5759, Cost Time: 42.04s\n",
      "Time: 2018-04-05 15:53:34.793000000~2018-04-05 16:09:23.048000000, Loss: 2.0498, Nodes_count: 5780, Cost Time: 42.61s\n",
      "Time: 2018-04-05 16:09:23.048000000~2018-04-05 16:25:55.195000000, Loss: 2.0952, Nodes_count: 5928, Cost Time: 43.65s\n",
      "Time: 2018-04-05 16:25:55.195000000~2018-04-05 16:41:03.167000000, Loss: 1.6682, Nodes_count: 5989, Cost Time: 45.28s\n",
      "Time: 2018-04-05 16:41:03.167000000~2018-04-05 16:59:07.920000000, Loss: 2.4712, Nodes_count: 6055, Cost Time: 45.87s\n",
      "Time: 2018-04-05 16:59:07.920000000~2018-04-05 17:15:19.588000000, Loss: 2.4595, Nodes_count: 6067, Cost Time: 46.02s\n",
      "Time: 2018-04-05 17:15:19.588000000~2018-04-05 17:35:14.078000000, Loss: 2.5208, Nodes_count: 6081, Cost Time: 46.14s\n",
      "Time: 2018-04-05 17:35:14.078000000~2018-04-05 17:55:16.875000000, Loss: 2.5274, Nodes_count: 6100, Cost Time: 46.26s\n",
      "Time: 2018-04-05 17:55:16.875000000~2018-04-05 18:15:42.341000000, Loss: 2.5276, Nodes_count: 6121, Cost Time: 46.38s\n",
      "Time: 2018-04-05 18:15:42.341000000~2018-04-05 18:37:14.681000000, Loss: 2.3622, Nodes_count: 6142, Cost Time: 46.56s\n",
      "Time: 2018-04-05 18:37:14.681000000~2018-04-05 18:58:00.402000000, Loss: 2.8668, Nodes_count: 6162, Cost Time: 46.68s\n",
      "Time: 2018-04-05 18:58:00.402000000~2018-04-05 19:17:05.754000000, Loss: 2.6093, Nodes_count: 6180, Cost Time: 46.81s\n",
      "Time: 2018-04-05 19:17:05.754000000~2018-04-05 19:37:53.928000000, Loss: 2.5829, Nodes_count: 6198, Cost Time: 46.93s\n",
      "Time: 2018-04-05 19:37:53.928000000~2018-04-05 19:59:18.826000000, Loss: 2.6241, Nodes_count: 6220, Cost Time: 47.05s\n",
      "Time: 2018-04-05 19:59:18.826000000~2018-04-05 20:19:00.511000000, Loss: 2.5684, Nodes_count: 6229, Cost Time: 47.17s\n",
      "Time: 2018-04-05 20:19:00.511000000~2018-04-05 20:39:00.399000000, Loss: 2.5108, Nodes_count: 6250, Cost Time: 47.29s\n",
      "Time: 2018-04-05 20:39:00.399000000~2018-04-05 21:00:00.321000000, Loss: 2.5321, Nodes_count: 6277, Cost Time: 47.42s\n",
      "Time: 2018-04-05 21:00:00.321000000~2018-04-05 21:19:46.983000000, Loss: 2.4986, Nodes_count: 6293, Cost Time: 47.59s\n",
      "Time: 2018-04-05 21:19:46.983000000~2018-04-05 21:38:46.760000000, Loss: 2.5200, Nodes_count: 6309, Cost Time: 47.77s\n",
      "Time: 2018-04-05 21:38:46.760000000~2018-04-05 22:00:00.344000000, Loss: 2.6703, Nodes_count: 6331, Cost Time: 47.90s\n",
      "Time: 2018-04-05 22:00:00.344000000~2018-04-05 22:19:18.475000000, Loss: 2.6362, Nodes_count: 6343, Cost Time: 48.08s\n",
      "Time: 2018-04-05 22:19:18.475000000~2018-04-05 22:39:21.047000000, Loss: 2.6426, Nodes_count: 6357, Cost Time: 48.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2018-04-05 22:39:21.047000000~2018-04-05 22:57:00.416000000, Loss: 2.4539, Nodes_count: 6387, Cost Time: 48.38s\n",
      "Time: 2018-04-05 22:57:00.416000000~2018-04-05 23:15:02.754000000, Loss: 2.5163, Nodes_count: 6403, Cost Time: 48.53s\n",
      "Time: 2018-04-05 23:15:02.754000000~2018-04-05 23:35:48.736000000, Loss: 2.5831, Nodes_count: 6418, Cost Time: 48.66s\n",
      "Time: 2018-04-05 23:35:48.736000000~2018-04-05 23:58:11.523000000, Loss: 2.5477, Nodes_count: 6439, Cost Time: 48.78s\n"
     ]
    }
   ],
   "source": [
    "ans_4_5=test_day_new(graph_4_5,\"graph_4_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1134670], msg=[1134670, 40], src=[1134670], t=[1134670])\n",
      "Time: 2018-04-06 00:00:00.050000000~2018-04-06 00:19:16.992000000, Loss: 3.0505, Nodes_count: 43, Cost Time: 0.11s\n",
      "Time: 2018-04-06 00:19:16.992000000~2018-04-06 00:45:05.853000000, Loss: 2.5677, Nodes_count: 87, Cost Time: 0.35s\n",
      "Time: 2018-04-06 00:45:05.853000000~2018-04-06 01:00:11.671000000, Loss: 2.7687, Nodes_count: 114, Cost Time: 0.48s\n",
      "Time: 2018-04-06 01:00:11.671000000~2018-04-06 01:18:57.427000000, Loss: 2.5952, Nodes_count: 126, Cost Time: 0.66s\n",
      "Time: 2018-04-06 01:18:57.427000000~2018-04-06 01:42:40.184000000, Loss: 2.6622, Nodes_count: 141, Cost Time: 0.78s\n",
      "Time: 2018-04-06 01:42:40.184000000~2018-04-06 02:05:25.221000000, Loss: 2.5541, Nodes_count: 176, Cost Time: 1.02s\n",
      "Time: 2018-04-06 02:05:25.221000000~2018-04-06 02:22:54.336000000, Loss: 2.5027, Nodes_count: 195, Cost Time: 1.20s\n",
      "Time: 2018-04-06 02:22:54.336000000~2018-04-06 02:44:45.281000000, Loss: 2.6663, Nodes_count: 225, Cost Time: 1.33s\n",
      "Time: 2018-04-06 02:44:45.281000000~2018-04-06 03:03:00.573000000, Loss: 2.5813, Nodes_count: 248, Cost Time: 1.45s\n",
      "Time: 2018-04-06 03:03:00.573000000~2018-04-06 03:25:24.762000000, Loss: 2.5871, Nodes_count: 266, Cost Time: 1.57s\n",
      "Time: 2018-04-06 03:25:24.762000000~2018-04-06 03:45:28.902000000, Loss: 2.4852, Nodes_count: 287, Cost Time: 1.69s\n",
      "Time: 2018-04-06 03:45:28.902000000~2018-04-06 04:05:31.388000000, Loss: 2.7184, Nodes_count: 310, Cost Time: 1.82s\n",
      "Time: 2018-04-06 04:05:31.388000000~2018-04-06 04:27:35.079000000, Loss: 2.6194, Nodes_count: 331, Cost Time: 1.94s\n",
      "Time: 2018-04-06 04:27:35.079000000~2018-04-06 04:45:32.422000000, Loss: 2.4561, Nodes_count: 353, Cost Time: 2.12s\n",
      "Time: 2018-04-06 04:45:32.422000000~2018-04-06 05:07:27.473000000, Loss: 2.7733, Nodes_count: 367, Cost Time: 2.24s\n",
      "Time: 2018-04-06 05:07:27.473000000~2018-04-06 05:27:24.984000000, Loss: 2.5341, Nodes_count: 391, Cost Time: 2.37s\n",
      "Time: 2018-04-06 05:27:24.984000000~2018-04-06 05:47:11.800000000, Loss: 2.4997, Nodes_count: 411, Cost Time: 2.49s\n",
      "Time: 2018-04-06 05:47:11.800000000~2018-04-06 06:05:34.537000000, Loss: 2.5992, Nodes_count: 430, Cost Time: 2.67s\n",
      "Time: 2018-04-06 06:05:34.537000000~2018-04-06 06:28:00.121000000, Loss: 2.7568, Nodes_count: 450, Cost Time: 2.79s\n",
      "Time: 2018-04-06 06:28:00.121000000~2018-04-06 06:45:02.339000000, Loss: 2.9566, Nodes_count: 469, Cost Time: 3.08s\n",
      "Time: 2018-04-06 06:45:02.339000000~2018-04-06 07:05:33.094000000, Loss: 2.6081, Nodes_count: 487, Cost Time: 3.21s\n",
      "Time: 2018-04-06 07:05:33.094000000~2018-04-06 07:27:38.006000000, Loss: 2.5811, Nodes_count: 511, Cost Time: 3.33s\n",
      "Time: 2018-04-06 07:27:38.006000000~2018-04-06 07:47:00.431000000, Loss: 2.5629, Nodes_count: 526, Cost Time: 3.45s\n",
      "Time: 2018-04-06 07:47:00.431000000~2018-04-06 08:07:33.934000000, Loss: 2.6123, Nodes_count: 546, Cost Time: 3.57s\n",
      "Time: 2018-04-06 08:07:33.934000000~2018-04-06 08:29:46.554000000, Loss: 2.5474, Nodes_count: 573, Cost Time: 3.70s\n",
      "Time: 2018-04-06 08:29:46.554000000~2018-04-06 08:45:16.620000000, Loss: 2.6022, Nodes_count: 594, Cost Time: 3.88s\n",
      "Time: 2018-04-06 08:45:16.620000000~2018-04-06 09:04:00.469000000, Loss: 2.5157, Nodes_count: 611, Cost Time: 4.00s\n",
      "Time: 2018-04-06 09:04:00.469000000~2018-04-06 09:29:00.411000000, Loss: 2.6811, Nodes_count: 640, Cost Time: 4.12s\n",
      "Time: 2018-04-06 09:29:00.411000000~2018-04-06 09:49:36.361000000, Loss: 2.9375, Nodes_count: 656, Cost Time: 4.30s\n",
      "Time: 2018-04-06 09:49:36.361000000~2018-04-06 10:11:25.827000000, Loss: 2.7180, Nodes_count: 679, Cost Time: 4.43s\n",
      "Time: 2018-04-06 10:11:25.827000000~2018-04-06 10:33:37.548000000, Loss: 2.7294, Nodes_count: 707, Cost Time: 4.55s\n",
      "Time: 2018-04-06 10:33:37.548000000~2018-04-06 10:51:59.967000000, Loss: 2.6146, Nodes_count: 725, Cost Time: 4.73s\n",
      "Time: 2018-04-06 10:51:59.967000000~2018-04-06 11:14:09.465000000, Loss: 2.7082, Nodes_count: 742, Cost Time: 4.85s\n",
      "Time: 2018-04-06 11:14:09.465000000~2018-04-06 11:30:58.081000000, Loss: 2.6406, Nodes_count: 772, Cost Time: 5.03s\n",
      "Time: 2018-04-06 11:30:58.081000000~2018-04-06 11:51:20.194000000, Loss: 2.5679, Nodes_count: 787, Cost Time: 5.16s\n",
      "Time: 2018-04-06 11:51:20.194000000~2018-04-06 12:09:43.911000000, Loss: 2.5165, Nodes_count: 806, Cost Time: 5.28s\n",
      "Time: 2018-04-06 12:09:43.911000000~2018-04-06 12:25:54.685000000, Loss: 2.8595, Nodes_count: 863, Cost Time: 5.52s\n",
      "Time: 2018-04-06 12:25:54.685000000~2018-04-06 12:41:20.717000000, Loss: 2.6416, Nodes_count: 919, Cost Time: 5.81s\n",
      "Time: 2018-04-06 12:41:20.717000000~2018-04-06 12:56:55.757000000, Loss: 2.7973, Nodes_count: 991, Cost Time: 6.11s\n",
      "Time: 2018-04-06 12:56:55.757000000~2018-04-06 13:12:04.238000000, Loss: 2.6321, Nodes_count: 1045, Cost Time: 6.41s\n",
      "Time: 2018-04-06 13:12:04.238000000~2018-04-06 13:30:04.328000000, Loss: 2.5620, Nodes_count: 1083, Cost Time: 6.60s\n",
      "Time: 2018-04-06 13:30:04.328000000~2018-04-06 13:45:17.542000000, Loss: 2.2589, Nodes_count: 1184, Cost Time: 7.01s\n",
      "Time: 2018-04-06 13:45:17.542000000~2018-04-06 14:00:35.220000000, Loss: 2.9702, Nodes_count: 1964, Cost Time: 8.22s\n",
      "Time: 2018-04-06 14:00:35.220000000~2018-04-06 14:19:19.282000000, Loss: 3.3479, Nodes_count: 2212, Cost Time: 8.81s\n",
      "Time: 2018-04-06 14:19:19.282000000~2018-04-06 14:36:21.009000000, Loss: 3.0000, Nodes_count: 2284, Cost Time: 9.23s\n",
      "Time: 2018-04-06 14:36:21.009000000~2018-04-06 14:52:35.840000000, Loss: 2.8021, Nodes_count: 2632, Cost Time: 10.17s\n",
      "Time: 2018-04-06 14:52:35.840000000~2018-04-06 15:08:30.414000000, Loss: 3.2381, Nodes_count: 2883, Cost Time: 10.90s\n",
      "Time: 2018-04-06 15:08:30.414000000~2018-04-06 15:29:50.869000000, Loss: 3.2774, Nodes_count: 2923, Cost Time: 11.17s\n",
      "Time: 2018-04-06 15:29:50.869000000~2018-04-06 15:49:26.015000000, Loss: 2.8536, Nodes_count: 3042, Cost Time: 11.75s\n",
      "Time: 2018-04-06 15:49:26.015000000~2018-04-06 16:05:10.837000000, Loss: 1.1916, Nodes_count: 3231, Cost Time: 16.92s\n",
      "Time: 2018-04-06 16:05:10.837000000~2018-04-06 16:20:11.626000000, Loss: 3.0613, Nodes_count: 3254, Cost Time: 17.35s\n",
      "Time: 2018-04-06 16:20:11.626000000~2018-04-06 16:35:25.587000000, Loss: 2.6026, Nodes_count: 4573, Cost Time: 21.60s\n",
      "Time: 2018-04-06 16:35:25.587000000~2018-04-06 16:50:32.945000000, Loss: 2.6014, Nodes_count: 4943, Cost Time: 24.13s\n",
      "Time: 2018-04-06 16:50:32.945000000~2018-04-06 17:05:39.288000000, Loss: 2.8743, Nodes_count: 5490, Cost Time: 25.52s\n",
      "Time: 2018-04-06 17:05:39.288000000~2018-04-06 17:20:54.977000000, Loss: 2.3792, Nodes_count: 6203, Cost Time: 29.43s\n",
      "Time: 2018-04-06 17:20:54.977000000~2018-04-06 17:36:52.502000000, Loss: 1.8065, Nodes_count: 7037, Cost Time: 33.73s\n",
      "Time: 2018-04-06 17:36:52.502000000~2018-04-06 17:52:05.798000000, Loss: 2.5121, Nodes_count: 7783, Cost Time: 35.87s\n",
      "Time: 2018-04-06 17:52:05.798000000~2018-04-06 18:12:57.421000000, Loss: 1.3015, Nodes_count: 7880, Cost Time: 39.23s\n",
      "Time: 2018-04-06 18:12:57.421000000~2018-04-06 18:30:22.483000000, Loss: 2.8375, Nodes_count: 8569, Cost Time: 40.81s\n",
      "Time: 2018-04-06 18:30:22.483000000~2018-04-06 18:45:23.209000000, Loss: 2.5128, Nodes_count: 10489, Cost Time: 45.26s\n",
      "Time: 2018-04-06 18:45:23.209000000~2018-04-06 19:04:54.592000000, Loss: 2.2360, Nodes_count: 11175, Cost Time: 52.34s\n",
      "Time: 2018-04-06 19:04:54.592000000~2018-04-06 19:23:03.611000000, Loss: 2.5954, Nodes_count: 12119, Cost Time: 57.11s\n",
      "Time: 2018-04-06 19:23:03.611000000~2018-04-06 19:44:32.667000000, Loss: 2.8137, Nodes_count: 12557, Cost Time: 58.36s\n",
      "Time: 2018-04-06 19:44:32.667000000~2018-04-06 20:00:39.987000000, Loss: 2.9805, Nodes_count: 12716, Cost Time: 58.99s\n",
      "Time: 2018-04-06 20:00:39.987000000~2018-04-06 20:19:42.681000000, Loss: 2.6429, Nodes_count: 12727, Cost Time: 59.20s\n",
      "Time: 2018-04-06 20:19:42.681000000~2018-04-06 20:42:46.845000000, Loss: 2.1788, Nodes_count: 12750, Cost Time: 59.77s\n",
      "Time: 2018-04-06 20:42:46.845000000~2018-04-06 20:59:25.771000000, Loss: 2.8717, Nodes_count: 13402, Cost Time: 60.74s\n",
      "Time: 2018-04-06 20:59:25.771000000~2018-04-06 21:28:30.420000000, Loss: 3.3975, Nodes_count: 13464, Cost Time: 60.95s\n",
      "Time: 2018-04-06 21:28:30.420000000~2018-04-06 21:53:32.222000000, Loss: 2.9791, Nodes_count: 13496, Cost Time: 61.08s\n",
      "Time: 2018-04-06 21:53:32.222000000~2018-04-06 22:10:33.999000000, Loss: 2.2717, Nodes_count: 14116, Cost Time: 64.14s\n",
      "Time: 2018-04-06 22:10:33.999000000~2018-04-06 22:39:24.718000000, Loss: 3.4383, Nodes_count: 14158, Cost Time: 64.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2018-04-06 22:39:24.718000000~2018-04-06 23:10:53.922000000, Loss: 2.7050, Nodes_count: 14572, Cost Time: 66.07s\n",
      "Time: 2018-04-06 23:10:53.922000000~2018-04-06 23:31:29.855000000, Loss: 3.4315, Nodes_count: 14605, Cost Time: 66.21s\n",
      "Time: 2018-04-06 23:31:29.855000000~2018-04-06 23:58:03.573000000, Loss: 3.3658, Nodes_count: 14628, Cost Time: 66.27s\n"
     ]
    }
   ],
   "source": [
    "ans_4_6=test_day_new(graph_4_6,\"graph_4_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1847921], msg=[1847921, 40], src=[1847921], t=[1847921])\n",
      "Time: 2018-04-07 00:00:00.040000000~2018-04-07 00:30:11.169000000, Loss: 3.3562, Nodes_count: 75, Cost Time: 0.12s\n",
      "Time: 2018-04-07 00:30:11.169000000~2018-04-07 00:55:07.465000000, Loss: 3.4510, Nodes_count: 99, Cost Time: 0.24s\n",
      "Time: 2018-04-07 00:55:07.465000000~2018-04-07 01:18:15.361000000, Loss: 3.2159, Nodes_count: 119, Cost Time: 0.30s\n",
      "Time: 2018-04-07 01:18:15.361000000~2018-04-07 01:44:15.385000000, Loss: 3.3938, Nodes_count: 142, Cost Time: 0.36s\n",
      "Time: 2018-04-07 01:44:15.385000000~2018-04-07 02:15:00.653000000, Loss: 3.2387, Nodes_count: 176, Cost Time: 0.48s\n",
      "Time: 2018-04-07 02:15:00.653000000~2018-04-07 02:30:22.992000000, Loss: 3.3829, Nodes_count: 201, Cost Time: 0.60s\n",
      "Time: 2018-04-07 02:30:22.992000000~2018-04-07 02:45:49.212000000, Loss: 3.4110, Nodes_count: 210, Cost Time: 0.67s\n",
      "Time: 2018-04-07 02:45:49.212000000~2018-04-07 03:11:00.395000000, Loss: 3.5733, Nodes_count: 227, Cost Time: 0.73s\n",
      "Time: 2018-04-07 03:11:00.395000000~2018-04-07 03:32:00.300000000, Loss: 3.4311, Nodes_count: 256, Cost Time: 0.80s\n",
      "Time: 2018-04-07 03:32:00.300000000~2018-04-07 03:58:51.900000000, Loss: 3.4557, Nodes_count: 277, Cost Time: 0.87s\n",
      "Time: 2018-04-07 03:58:51.900000000~2018-04-07 04:23:34.116000000, Loss: 3.1624, Nodes_count: 300, Cost Time: 0.93s\n",
      "Time: 2018-04-07 04:23:34.116000000~2018-04-07 04:46:00.626000000, Loss: 3.0298, Nodes_count: 319, Cost Time: 0.99s\n",
      "Time: 2018-04-07 04:46:00.626000000~2018-04-07 05:10:42.317000000, Loss: 3.2481, Nodes_count: 333, Cost Time: 1.05s\n",
      "Time: 2018-04-07 05:10:42.317000000~2018-04-07 05:30:07.852000000, Loss: 3.4108, Nodes_count: 364, Cost Time: 1.12s\n",
      "Time: 2018-04-07 05:30:07.852000000~2018-04-07 05:57:05.858000000, Loss: 4.0910, Nodes_count: 378, Cost Time: 1.24s\n",
      "Time: 2018-04-07 05:57:05.858000000~2018-04-07 06:20:15.841000000, Loss: 3.1308, Nodes_count: 398, Cost Time: 1.30s\n",
      "Time: 2018-04-07 06:20:15.841000000~2018-04-07 06:44:17.080000000, Loss: 3.2399, Nodes_count: 419, Cost Time: 1.36s\n",
      "Time: 2018-04-07 06:44:17.080000000~2018-04-07 07:08:21.521000000, Loss: 3.3528, Nodes_count: 440, Cost Time: 1.43s\n",
      "Time: 2018-04-07 07:08:21.521000000~2018-04-07 07:30:13.991000000, Loss: 3.4834, Nodes_count: 468, Cost Time: 1.49s\n",
      "Time: 2018-04-07 07:30:13.991000000~2018-04-07 07:54:56.858000000, Loss: 3.3468, Nodes_count: 492, Cost Time: 1.55s\n",
      "Time: 2018-04-07 07:54:56.858000000~2018-04-07 08:17:34.639000000, Loss: 3.3536, Nodes_count: 514, Cost Time: 1.62s\n",
      "Time: 2018-04-07 08:17:34.639000000~2018-04-07 08:34:49.687000000, Loss: 3.5510, Nodes_count: 537, Cost Time: 1.74s\n",
      "Time: 2018-04-07 08:34:49.687000000~2018-04-07 08:58:17.722000000, Loss: 3.5010, Nodes_count: 561, Cost Time: 1.80s\n",
      "Time: 2018-04-07 08:58:17.722000000~2018-04-07 09:13:29.714000000, Loss: 1.1539, Nodes_count: 700, Cost Time: 5.95s\n",
      "Time: 2018-04-07 09:13:29.714000000~2018-04-07 09:28:31.696000000, Loss: 0.9586, Nodes_count: 772, Cost Time: 11.75s\n",
      "Time: 2018-04-07 09:28:31.696000000~2018-04-07 09:45:27.476000000, Loss: 0.9897, Nodes_count: 816, Cost Time: 18.00s\n",
      "Time: 2018-04-07 09:45:27.476000000~2018-04-07 10:00:33.713000000, Loss: 3.6472, Nodes_count: 835, Cost Time: 18.42s\n",
      "Time: 2018-04-07 10:00:33.713000000~2018-04-07 10:22:14.290000000, Loss: 2.1961, Nodes_count: 1542, Cost Time: 21.19s\n",
      "Time: 2018-04-07 10:22:14.290000000~2018-04-07 10:39:58.094000000, Loss: 2.4174, Nodes_count: 2469, Cost Time: 23.77s\n",
      "Time: 2018-04-07 10:39:58.094000000~2018-04-07 11:00:08.471000000, Loss: 1.4445, Nodes_count: 3246, Cost Time: 29.39s\n",
      "Time: 2018-04-07 11:00:08.471000000~2018-04-07 11:15:32.309000000, Loss: 2.0309, Nodes_count: 3821, Cost Time: 32.54s\n",
      "Time: 2018-04-07 11:15:32.309000000~2018-04-07 11:35:46.576000000, Loss: 1.8381, Nodes_count: 4297, Cost Time: 35.95s\n",
      "Time: 2018-04-07 11:35:46.576000000~2018-04-07 11:51:38.305000000, Loss: 3.0442, Nodes_count: 4381, Cost Time: 36.86s\n",
      "Time: 2018-04-07 11:51:38.305000000~2018-04-07 12:06:57.429000000, Loss: 1.1115, Nodes_count: 4472, Cost Time: 42.50s\n",
      "Time: 2018-04-07 12:06:57.429000000~2018-04-07 12:21:58.368000000, Loss: 2.8321, Nodes_count: 4623, Cost Time: 43.51s\n",
      "Time: 2018-04-07 12:21:58.368000000~2018-04-07 12:37:22.947000000, Loss: 2.8416, Nodes_count: 5239, Cost Time: 45.04s\n",
      "Time: 2018-04-07 12:37:22.947000000~2018-04-07 12:52:26.920000000, Loss: 1.4487, Nodes_count: 6067, Cost Time: 50.95s\n",
      "Time: 2018-04-07 12:52:26.920000000~2018-04-07 13:14:42.953000000, Loss: 2.2188, Nodes_count: 6329, Cost Time: 52.43s\n",
      "Time: 2018-04-07 13:14:42.953000000~2018-04-07 13:41:49.780000000, Loss: 3.1890, Nodes_count: 6352, Cost Time: 52.60s\n",
      "Time: 2018-04-07 13:41:49.780000000~2018-04-07 14:00:00.583000000, Loss: 3.1730, Nodes_count: 6413, Cost Time: 52.84s\n",
      "Time: 2018-04-07 14:00:00.583000000~2018-04-07 14:15:33.098000000, Loss: 2.1588, Nodes_count: 7312, Cost Time: 56.98s\n",
      "Time: 2018-04-07 14:15:33.098000000~2018-04-07 14:37:39.333000000, Loss: 2.1713, Nodes_count: 7844, Cost Time: 59.96s\n",
      "Time: 2018-04-07 14:37:39.333000000~2018-04-07 14:53:20.642000000, Loss: 3.2553, Nodes_count: 7871, Cost Time: 60.33s\n",
      "Time: 2018-04-07 14:53:20.642000000~2018-04-07 15:08:21.119000000, Loss: 1.3472, Nodes_count: 7938, Cost Time: 61.69s\n",
      "Time: 2018-04-07 15:08:21.119000000~2018-04-07 15:24:08.091000000, Loss: 0.9593, Nodes_count: 7950, Cost Time: 65.35s\n",
      "Time: 2018-04-07 15:24:08.091000000~2018-04-07 15:39:13.623000000, Loss: 1.1943, Nodes_count: 8026, Cost Time: 67.87s\n",
      "Time: 2018-04-07 15:39:13.623000000~2018-04-07 16:00:04.364000000, Loss: 1.2561, Nodes_count: 8070, Cost Time: 68.89s\n",
      "Time: 2018-04-07 16:00:04.364000000~2018-04-07 16:25:54.546000000, Loss: 3.7352, Nodes_count: 8424, Cost Time: 69.79s\n",
      "Time: 2018-04-07 16:25:54.546000000~2018-04-07 16:42:27.814000000, Loss: 3.2297, Nodes_count: 8432, Cost Time: 69.90s\n",
      "Time: 2018-04-07 16:42:27.814000000~2018-04-07 16:57:38.063000000, Loss: 1.1521, Nodes_count: 8527, Cost Time: 72.47s\n",
      "Time: 2018-04-07 16:57:38.063000000~2018-04-07 17:12:59.921000000, Loss: 2.3939, Nodes_count: 9175, Cost Time: 75.37s\n",
      "Time: 2018-04-07 17:12:59.921000000~2018-04-07 17:30:04.373000000, Loss: 2.6408, Nodes_count: 9731, Cost Time: 77.99s\n",
      "Time: 2018-04-07 17:30:04.373000000~2018-04-07 17:45:12.921000000, Loss: 2.0698, Nodes_count: 9826, Cost Time: 79.42s\n",
      "Time: 2018-04-07 17:45:12.921000000~2018-04-07 18:00:33.420000000, Loss: 2.9806, Nodes_count: 10730, Cost Time: 82.66s\n",
      "Time: 2018-04-07 18:00:33.420000000~2018-04-07 18:24:44.896000000, Loss: 2.2154, Nodes_count: 10749, Cost Time: 83.28s\n",
      "Time: 2018-04-07 18:24:44.896000000~2018-04-07 18:39:52.248000000, Loss: 2.7301, Nodes_count: 11190, Cost Time: 84.72s\n",
      "Time: 2018-04-07 18:39:52.248000000~2018-04-07 18:56:09.290000000, Loss: 1.2180, Nodes_count: 11300, Cost Time: 87.75s\n",
      "Time: 2018-04-07 18:56:09.290000000~2018-04-07 19:21:04.391000000, Loss: 3.6130, Nodes_count: 11330, Cost Time: 88.01s\n",
      "Time: 2018-04-07 19:21:04.391000000~2018-04-07 19:45:02.042000000, Loss: 3.3163, Nodes_count: 11352, Cost Time: 88.08s\n",
      "Time: 2018-04-07 19:45:02.042000000~2018-04-07 20:00:59.751000000, Loss: 3.3866, Nodes_count: 11389, Cost Time: 88.20s\n",
      "Time: 2018-04-07 20:00:59.751000000~2018-04-07 20:21:50.560000000, Loss: 3.4368, Nodes_count: 11503, Cost Time: 88.55s\n",
      "Time: 2018-04-07 20:21:50.560000000~2018-04-07 20:37:54.893000000, Loss: 3.4854, Nodes_count: 12079, Cost Time: 89.87s\n",
      "Time: 2018-04-07 20:37:54.893000000~2018-04-07 20:52:58.485000000, Loss: 2.5116, Nodes_count: 12858, Cost Time: 95.37s\n",
      "Time: 2018-04-07 20:52:58.485000000~2018-04-07 21:08:28.188000000, Loss: 2.0238, Nodes_count: 12892, Cost Time: 102.44s\n",
      "Time: 2018-04-07 21:08:28.188000000~2018-04-07 21:24:36.756000000, Loss: 2.2514, Nodes_count: 13376, Cost Time: 105.99s\n",
      "Time: 2018-04-07 21:24:36.756000000~2018-04-07 21:42:26.573000000, Loss: 3.2204, Nodes_count: 13379, Cost Time: 106.22s\n",
      "Time: 2018-04-07 21:42:26.573000000~2018-04-07 22:05:45.793000000, Loss: 3.3308, Nodes_count: 13426, Cost Time: 106.39s\n",
      "Time: 2018-04-07 22:05:45.793000000~2018-04-07 22:30:10.178000000, Loss: 3.4115, Nodes_count: 13468, Cost Time: 106.52s\n",
      "Time: 2018-04-07 22:30:10.178000000~2018-04-07 22:47:23.900000000, Loss: 3.2763, Nodes_count: 13481, Cost Time: 106.64s\n",
      "Time: 2018-04-07 22:47:23.900000000~2018-04-07 23:22:01.561000000, Loss: 3.3569, Nodes_count: 13519, Cost Time: 106.82s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2018-04-07 23:22:01.561000000~2018-04-07 23:44:24.554000000, Loss: 3.4114, Nodes_count: 13543, Cost Time: 106.89s\n",
      "Time: 2018-04-07 23:44:24.554000000~2018-04-07 23:59:58.135000000, Loss: 3.4577, Nodes_count: 13557, Cost Time: 106.93s\n"
     ]
    }
   ],
   "source": [
    "ans_4_7=test_day_new(graph_4_7,\"graph_4_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[2554245], msg=[2554245, 40], src=[2554245], t=[2554245])\n",
      "Time: 2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000, Loss: 3.0210, Nodes_count: 81, Cost Time: 0.06s\n",
      "Time: 2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000, Loss: 3.8966, Nodes_count: 156, Cost Time: 0.12s\n",
      "Time: 2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000, Loss: 3.7006, Nodes_count: 226, Cost Time: 0.18s\n",
      "Time: 2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000, Loss: 3.7235, Nodes_count: 300, Cost Time: 0.25s\n",
      "Time: 2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000, Loss: 4.0117, Nodes_count: 367, Cost Time: 0.31s\n",
      "Time: 2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000, Loss: 3.6330, Nodes_count: 442, Cost Time: 0.38s\n",
      "Time: 2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000, Loss: 2.9148, Nodes_count: 1100, Cost Time: 2.31s\n",
      "Time: 2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000, Loss: 2.8995, Nodes_count: 1151, Cost Time: 3.15s\n",
      "Time: 2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000, Loss: 2.3967, Nodes_count: 2173, Cost Time: 6.13s\n",
      "Time: 2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000, Loss: 1.1400, Nodes_count: 2462, Cost Time: 12.87s\n",
      "Time: 2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000, Loss: 0.9445, Nodes_count: 2505, Cost Time: 20.34s\n",
      "Time: 2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000, Loss: 1.2016, Nodes_count: 2560, Cost Time: 21.71s\n",
      "Time: 2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000, Loss: 2.5588, Nodes_count: 2925, Cost Time: 23.32s\n",
      "Time: 2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000, Loss: 2.1602, Nodes_count: 3671, Cost Time: 27.71s\n",
      "Time: 2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000, Loss: 2.9559, Nodes_count: 4097, Cost Time: 29.53s\n",
      "Time: 2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000, Loss: 1.6098, Nodes_count: 4210, Cost Time: 32.22s\n",
      "Time: 2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000, Loss: 1.1428, Nodes_count: 4385, Cost Time: 37.07s\n",
      "Time: 2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000, Loss: 1.3848, Nodes_count: 4417, Cost Time: 37.93s\n",
      "Time: 2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000, Loss: 1.3024, Nodes_count: 4512, Cost Time: 40.77s\n",
      "Time: 2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000, Loss: 2.4968, Nodes_count: 4737, Cost Time: 41.49s\n",
      "Time: 2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000, Loss: 1.3088, Nodes_count: 4848, Cost Time: 43.77s\n",
      "Time: 2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000, Loss: 2.1413, Nodes_count: 4939, Cost Time: 44.69s\n",
      "Time: 2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000, Loss: 2.2504, Nodes_count: 5614, Cost Time: 47.34s\n",
      "Time: 2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000, Loss: 3.2245, Nodes_count: 5769, Cost Time: 48.27s\n",
      "Time: 2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000, Loss: 2.4217, Nodes_count: 6670, Cost Time: 52.05s\n",
      "Time: 2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000, Loss: 2.4436, Nodes_count: 6749, Cost Time: 80.24s\n",
      "Time: 2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000, Loss: 1.1659, Nodes_count: 6832, Cost Time: 85.63s\n",
      "Time: 2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000, Loss: 1.4280, Nodes_count: 7389, Cost Time: 91.96s\n",
      "Time: 2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000, Loss: 2.8275, Nodes_count: 8060, Cost Time: 94.71s\n",
      "Time: 2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000, Loss: 2.4857, Nodes_count: 8142, Cost Time: 96.09s\n",
      "Time: 2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000, Loss: 1.1576, Nodes_count: 8256, Cost Time: 101.59s\n",
      "Time: 2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000, Loss: 2.8524, Nodes_count: 8459, Cost Time: 102.78s\n",
      "Time: 2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000, Loss: 1.1187, Nodes_count: 8595, Cost Time: 108.82s\n",
      "Time: 2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000, Loss: 3.0896, Nodes_count: 9434, Cost Time: 110.84s\n",
      "Time: 2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000, Loss: 2.7804, Nodes_count: 9567, Cost Time: 111.40s\n",
      "Time: 2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000, Loss: 2.7104, Nodes_count: 9787, Cost Time: 113.07s\n",
      "Time: 2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000, Loss: 2.1634, Nodes_count: 10062, Cost Time: 115.60s\n",
      "Time: 2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000, Loss: 2.1695, Nodes_count: 10664, Cost Time: 119.64s\n",
      "Time: 2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000, Loss: 1.5227, Nodes_count: 10929, Cost Time: 122.94s\n",
      "Time: 2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000, Loss: 2.0512, Nodes_count: 11314, Cost Time: 126.60s\n",
      "Time: 2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000, Loss: 2.8282, Nodes_count: 11373, Cost Time: 127.23s\n",
      "Time: 2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000, Loss: 2.7819, Nodes_count: 11877, Cost Time: 129.14s\n",
      "Time: 2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000, Loss: 1.1157, Nodes_count: 12019, Cost Time: 135.56s\n",
      "Time: 2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000, Loss: 1.3367, Nodes_count: 12080, Cost Time: 137.89s\n",
      "Time: 2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000, Loss: 2.6709, Nodes_count: 12515, Cost Time: 139.19s\n",
      "Time: 2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000, Loss: 3.1092, Nodes_count: 12685, Cost Time: 139.66s\n",
      "Time: 2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000, Loss: 3.5198, Nodes_count: 12812, Cost Time: 139.92s\n",
      "Time: 2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000, Loss: 3.6232, Nodes_count: 12983, Cost Time: 140.22s\n",
      "Time: 2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000, Loss: 2.5760, Nodes_count: 13943, Cost Time: 144.71s\n",
      "Time: 2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000, Loss: 3.3135, Nodes_count: 13975, Cost Time: 145.00s\n",
      "Time: 2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000, Loss: 2.5215, Nodes_count: 14407, Cost Time: 146.48s\n",
      "Time: 2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000, Loss: 2.9392, Nodes_count: 14450, Cost Time: 146.79s\n",
      "Time: 2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000, Loss: 2.1016, Nodes_count: 14478, Cost Time: 146.92s\n",
      "Time: 2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000, Loss: 2.0966, Nodes_count: 14503, Cost Time: 147.55s\n",
      "Time: 2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000, Loss: 2.5467, Nodes_count: 14520, Cost Time: 147.64s\n",
      "Time: 2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000, Loss: 2.6007, Nodes_count: 14550, Cost Time: 147.82s\n",
      "Time: 2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000, Loss: 4.2620, Nodes_count: 14606, Cost Time: 147.85s\n"
     ]
    }
   ],
   "source": [
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge: TemporalData(dst=[1976440], msg=[1976440, 40], src=[1976440], t=[1976440])\n",
      "Time: 2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000, Loss: 3.2503, Nodes_count: 127, Cost Time: 0.06s\n",
      "Time: 2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000, Loss: 3.9492, Nodes_count: 230, Cost Time: 0.12s\n",
      "Time: 2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000, Loss: 4.1375, Nodes_count: 333, Cost Time: 0.19s\n",
      "Time: 2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000, Loss: 3.8054, Nodes_count: 439, Cost Time: 0.25s\n",
      "Time: 2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000, Loss: 2.6153, Nodes_count: 482, Cost Time: 0.48s\n",
      "Time: 2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000, Loss: 2.9197, Nodes_count: 514, Cost Time: 0.61s\n",
      "Time: 2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000, Loss: 2.9916, Nodes_count: 548, Cost Time: 0.68s\n",
      "Time: 2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000, Loss: 2.9103, Nodes_count: 744, Cost Time: 1.88s\n",
      "Time: 2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000, Loss: 2.7207, Nodes_count: 786, Cost Time: 2.06s\n",
      "Time: 2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000, Loss: 1.8055, Nodes_count: 891, Cost Time: 2.64s\n",
      "Time: 2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000, Loss: 2.5133, Nodes_count: 2594, Cost Time: 7.22s\n",
      "Time: 2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000, Loss: 2.7335, Nodes_count: 2661, Cost Time: 7.79s\n",
      "Time: 2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000, Loss: 1.9541, Nodes_count: 3334, Cost Time: 10.59s\n",
      "Time: 2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000, Loss: 1.7115, Nodes_count: 3515, Cost Time: 12.44s\n",
      "Time: 2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000, Loss: 2.4062, Nodes_count: 3752, Cost Time: 14.91s\n",
      "Time: 2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000, Loss: 3.7872, Nodes_count: 4436, Cost Time: 24.38s\n",
      "Time: 2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000, Loss: 1.9363, Nodes_count: 4597, Cost Time: 27.83s\n",
      "Time: 2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000, Loss: 4.1830, Nodes_count: 4831, Cost Time: 29.80s\n",
      "Time: 2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000, Loss: 3.1091, Nodes_count: 5587, Cost Time: 32.41s\n",
      "Time: 2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000, Loss: 2.1724, Nodes_count: 5777, Cost Time: 34.07s\n",
      "Time: 2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000, Loss: 3.1749, Nodes_count: 6163, Cost Time: 37.17s\n",
      "Time: 2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000, Loss: 4.2257, Nodes_count: 6619, Cost Time: 38.80s\n",
      "Time: 2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000, Loss: 3.7977, Nodes_count: 6680, Cost Time: 38.99s\n",
      "Time: 2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000, Loss: 4.6342, Nodes_count: 6832, Cost Time: 41.93s\n",
      "Time: 2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000, Loss: 4.3163, Nodes_count: 7500, Cost Time: 44.46s\n",
      "Time: 2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000, Loss: 4.2539, Nodes_count: 8579, Cost Time: 46.16s\n",
      "Time: 2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000, Loss: 4.5517, Nodes_count: 9592, Cost Time: 48.25s\n",
      "Time: 2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000, Loss: 4.6358, Nodes_count: 10337, Cost Time: 50.39s\n",
      "Time: 2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000, Loss: 5.2240, Nodes_count: 11153, Cost Time: 53.59s\n",
      "Time: 2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000, Loss: 4.7860, Nodes_count: 12116, Cost Time: 56.06s\n",
      "Time: 2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000, Loss: 4.8808, Nodes_count: 13192, Cost Time: 58.56s\n",
      "Time: 2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000, Loss: 5.2944, Nodes_count: 13440, Cost Time: 60.73s\n",
      "Time: 2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000, Loss: 4.5091, Nodes_count: 13870, Cost Time: 62.83s\n",
      "Time: 2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000, Loss: 4.4943, Nodes_count: 14026, Cost Time: 63.58s\n",
      "Time: 2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000, Loss: 5.5569, Nodes_count: 14174, Cost Time: 64.81s\n",
      "Time: 2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000, Loss: 4.4430, Nodes_count: 14762, Cost Time: 66.51s\n",
      "Time: 2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000, Loss: 4.3718, Nodes_count: 15025, Cost Time: 67.77s\n",
      "Time: 2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000, Loss: 4.3847, Nodes_count: 15746, Cost Time: 69.45s\n",
      "Time: 2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000, Loss: 4.3253, Nodes_count: 16251, Cost Time: 71.17s\n",
      "Time: 2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000, Loss: 3.7245, Nodes_count: 16933, Cost Time: 72.87s\n",
      "Time: 2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000, Loss: 3.6913, Nodes_count: 17438, Cost Time: 74.74s\n",
      "Time: 2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000, Loss: 4.3768, Nodes_count: 17966, Cost Time: 76.59s\n",
      "Time: 2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000, Loss: 4.5497, Nodes_count: 18466, Cost Time: 78.43s\n",
      "Time: 2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000, Loss: 4.7433, Nodes_count: 19017, Cost Time: 80.34s\n",
      "Time: 2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000, Loss: 2.0380, Nodes_count: 19268, Cost Time: 84.55s\n",
      "Time: 2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000, Loss: 4.6258, Nodes_count: 19822, Cost Time: 86.61s\n",
      "Time: 2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000, Loss: 4.1888, Nodes_count: 20424, Cost Time: 88.28s\n",
      "Time: 2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000, Loss: 4.4136, Nodes_count: 20887, Cost Time: 90.41s\n",
      "Time: 2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000, Loss: 3.0264, Nodes_count: 21288, Cost Time: 93.06s\n",
      "Time: 2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000, Loss: 4.4448, Nodes_count: 21834, Cost Time: 95.04s\n",
      "Time: 2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000, Loss: 4.5557, Nodes_count: 22117, Cost Time: 96.88s\n",
      "Time: 2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000, Loss: 1.9332, Nodes_count: 22349, Cost Time: 101.16s\n",
      "Time: 2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000, Loss: 4.4053, Nodes_count: 22587, Cost Time: 102.18s\n",
      "Time: 2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000, Loss: 4.3007, Nodes_count: 23551, Cost Time: 104.03s\n",
      "Time: 2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000, Loss: 4.0949, Nodes_count: 23854, Cost Time: 105.16s\n",
      "Time: 2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000, Loss: 4.3786, Nodes_count: 24180, Cost Time: 106.15s\n",
      "Time: 2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000, Loss: 2.5011, Nodes_count: 24644, Cost Time: 108.59s\n",
      "Time: 2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000, Loss: 3.9473, Nodes_count: 25119, Cost Time: 110.22s\n",
      "Time: 2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000, Loss: 2.0938, Nodes_count: 25398, Cost Time: 113.13s\n",
      "Time: 2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000, Loss: 4.6564, Nodes_count: 26030, Cost Time: 115.31s\n",
      "Time: 2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000, Loss: 4.3067, Nodes_count: 26346, Cost Time: 116.59s\n",
      "Time: 2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000, Loss: 4.2200, Nodes_count: 26396, Cost Time: 116.71s\n"
     ]
    }
   ],
   "source": [
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the node IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 293/293 [01:48<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF weight calculate complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "node_IDF={}\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF    \n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF\")\n",
    "print(\"IDF weight calculate complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+1.5*loss_std\n",
    "\n",
    "    print(\"thr:\",thr)\n",
    "\n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "            \n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/(count + 0.00001) ,node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the relations between time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "\n",
    "        'glx_alsa_675',\n",
    "        '/data/system/',\n",
    "         '/storage/emulated/',\n",
    "        '/data/data/com.android',\n",
    "        '/proc/',\n",
    "        'nz9885vc.default',\n",
    "      \n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "def cal_set_rel(s1,s2,node_IDF, file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if is_include_key_word(i) is False :\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "            if IDF>6:\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def cal_set_rel(s1,s2,node_IDF, file_list, node_IDF_4_4_7, file_list_4_4_7):\n",
    "#     new_s=s1 & s2\n",
    "#     count=0\n",
    "#     for i in new_s:\n",
    "# #     jdata=json.loads(i)\n",
    "#         if 'netflow' not in i and 'glx_alsa_675' not in i and '/data/system/' not in i and '/storage/emulated/' not in i and  '/data/data/com.android' not in i and  '/proc/' not in i and 'nz9885vc.default' not in i :\n",
    "\n",
    "# #         'netflow' not in i\n",
    "# #         and 'usr' not in i and 'var' not in i\n",
    "#             if i in node_IDF.keys():\n",
    "#                 IDF=node_IDF[i]\n",
    "#             else:\n",
    "#                 IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "#             if i in node_IDF_4_4_7.keys():\n",
    "#                 IDF4=node_IDF_4_4_7[i]\n",
    "#             else:\n",
    "#                 IDF4=math.log(len(file_list_4_4_7)/(1))    \n",
    "            \n",
    "# #             print(IDF)\n",
    "#             if (IDF+IDF4)>9:\n",
    "#                 print(\"node:\",i,\" IDF:\",IDF+IDF4)\n",
    "#                 count+=1\n",
    "#     return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "    \n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label={}\n",
    "\n",
    "filelist = os.listdir(\"graph_4_10/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_10/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_11/\")\n",
    "for f in filelist:\n",
    "    pred_label[\"graph_4_11/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_6/\"\n",
    "file_l=os.listdir(\"graph_4_6/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_4_7/\"\n",
    "file_l=os.listdir(\"graph_4_7/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 3.752937846735092\n",
      "2018-04-10 00:00:00.041000000~2018-04-10 01:14:00.502000000.txt    4.251545197476028  count: 60  percentage: 0.05859375  node count: 2  edge count: 1\n",
      "index_count: 1\n",
      "thr: 6.194344672236004\n",
      "2018-04-10 01:14:00.502000000~2018-04-10 02:24:00.583000000.txt    6.887248389067847  count: 103  percentage: 0.1005859375  node count: 16  edge count: 15\n",
      "index_count: 2\n",
      "thr: 6.206497046586859\n",
      "2018-04-10 02:24:00.583000000~2018-04-10 03:30:37.185000000.txt    6.976111277988085  count: 122  percentage: 0.119140625  node count: 15  edge count: 14\n",
      "index_count: 3\n",
      "thr: 5.94836398211992\n",
      "2018-04-10 03:30:37.185000000~2018-04-10 04:51:00.454000000.txt    6.793852028291493  count: 118  percentage: 0.115234375  node count: 16  edge count: 15\n",
      "index_count: 4\n",
      "thr: 6.390375697990951\n",
      "2018-04-10 04:51:00.454000000~2018-04-10 06:00:07.009000000.txt    7.114718570180262  count: 114  percentage: 0.111328125  node count: 18  edge count: 17\n",
      "index_count: 5\n",
      "thr: 5.654894072081403\n",
      "2018-04-10 06:00:07.009000000~2018-04-10 07:04:52.138000000.txt    6.633447851584406  count: 107  percentage: 0.1044921875  node count: 16  edge count: 15\n",
      "index_count: 6\n",
      "thr: 5.318096767450314\n",
      "2018-04-10 07:04:52.138000000~2018-04-10 07:19:59.157000000.txt    6.347645818019034  count: 2243  percentage: 0.06442440257352941  node count: 390  edge count: 388\n",
      "index_count: 7\n",
      "thr: 4.693086802495545\n",
      "2018-04-10 07:19:59.157000000~2018-04-10 07:52:43.143000000.txt    5.7945004605969554  count: 163  percentage: 0.012244591346153846  node count: 72  edge count: 68\n",
      "index_count: 8\n",
      "thr: 5.0698341450322335\n",
      "2018-04-10 07:52:43.143000000~2018-04-10 08:11:35.864000000.txt    6.26252663538639  count: 5106  percentage: 0.09589092548076923  node count: 766  edge count: 767\n",
      "index_count: 9\n",
      "thr: 2.5465242969319473\n",
      "2018-04-10 08:11:35.864000000~2018-04-10 08:33:00.367000000.txt    4.316361976181078  count: 8049  percentage: 0.06661314883474577  node count: 367  edge count: 389\n",
      "index_count: 10\n",
      "thr: 1.7114811877145835\n",
      "2018-04-10 08:33:00.367000000~2018-04-10 08:48:00.599000000.txt    4.390400424034728  count: 2093  percentage: 0.01596832275390625  node count: 108  edge count: 122\n",
      "index_count: 11\n",
      "thr: 3.1064767379655622\n",
      "2018-04-10 08:48:00.599000000~2018-04-10 09:03:00.861000000.txt    4.772102542288503  count: 1557  percentage: 0.08447265625  node count: 119  edge count: 130\n",
      "index_count: 12\n",
      "thr: 5.181035462571151\n",
      "2018-04-10 09:03:00.861000000~2018-04-10 09:19:34.766000000.txt    6.199488253607154  count: 2689  percentage: 0.0972583912037037  node count: 324  edge count: 328\n",
      "index_count: 13\n",
      "thr: 4.923989379728548\n",
      "2018-04-10 09:19:34.766000000~2018-04-10 09:45:04.552000000.txt    5.921169045790636  count: 9054  percentage: 0.11633943256578948  node count: 553  edge count: 561\n",
      "index_count: 14\n",
      "thr: 5.746019702761279\n",
      "2018-04-10 09:45:04.552000000~2018-04-10 10:06:29.544000000.txt    6.585972505404971  count: 2684  percentage: 0.09361049107142858  node count: 465  edge count: 460\n",
      "index_count: 15\n",
      "thr: 3.8220420188478146\n",
      "2018-04-10 10:06:29.544000000~2018-04-10 10:21:32.315000000.txt    5.12281987437658  count: 4569  percentage: 0.09699813179347826  node count: 216  edge count: 234\n",
      "index_count: 16\n",
      "thr: 2.856723533347054\n",
      "2018-04-10 10:21:32.315000000~2018-04-10 10:36:35.907000000.txt    4.814149614272669  count: 6023  percentage: 0.07002185639880952  node count: 266  edge count: 310\n",
      "index_count: 17\n",
      "thr: 3.044357113346014\n",
      "2018-04-10 10:36:35.907000000~2018-04-10 10:53:14.920000000.txt    4.549878162966569  count: 948  percentage: 0.08416193181818182  node count: 99  edge count: 112\n",
      "index_count: 18\n",
      "thr: 2.998692955733982\n",
      "2018-04-10 10:53:14.920000000~2018-04-10 11:12:35.572000000.txt    4.69095989986902  count: 3385  percentage: 0.06611328125  node count: 216  edge count: 235\n",
      "index_count: 19\n",
      "thr: 4.801048301054053\n",
      "2018-04-10 11:12:35.572000000~2018-04-10 11:28:38.819000000.txt    5.520310121078718  count: 981  percentage: 0.09580078125  node count: 302  edge count: 307\n",
      "index_count: 20\n",
      "thr: 3.203401210379292\n",
      "2018-04-10 11:28:38.819000000~2018-04-10 11:43:48.286000000.txt    4.825416061094256  count: 3702  percentage: 0.090380859375  node count: 192  edge count: 214\n",
      "index_count: 21\n",
      "thr: 4.65040660611651\n",
      "2018-04-10 11:43:48.286000000~2018-04-10 12:05:28.835000000.txt    5.8085018604240615  count: 1547  percentage: 0.10791015625  node count: 366  edge count: 405\n",
      "index_count: 22\n",
      "thr: 4.912888451299333\n",
      "2018-04-10 12:05:28.835000000~2018-04-10 12:21:06.782000000.txt    5.969212859996796  count: 4446  percentage: 0.09438688858695653  node count: 837  edge count: 895\n",
      "index_count: 23\n",
      "thr: 5.31722471500041\n",
      "2018-04-10 12:21:06.782000000~2018-04-10 12:45:20.100000000.txt    6.133685887607333  count: 1271  percentage: 0.08865792410714286  node count: 127  edge count: 126\n",
      "index_count: 24\n",
      "thr: 4.975132930528215\n",
      "2018-04-10 12:45:20.100000000~2018-04-10 13:02:56.955000000.txt    5.811453948931847  count: 6635  percentage: 0.0981741240530303  node count: 759  edge count: 757\n",
      "index_count: 25\n",
      "thr: 3.5527557416479714\n",
      "2018-04-10 13:02:56.955000000~2018-04-10 13:18:18.179000000.txt    5.172607901298968  count: 2677  percentage: 0.0052600760814889335  node count: 175  edge count: 196\n",
      "index_count: 26\n",
      "thr: 2.6233283707316666\n",
      "2018-04-10 13:18:18.179000000~2018-04-10 13:34:14.316000000.txt    3.945102263380473  count: 5756  percentage: 0.07807074652777778  node count: 176  edge count: 209\n",
      "index_count: 27\n",
      "thr: 3.388824447173822\n",
      "2018-04-10 13:34:14.316000000~2018-04-10 13:49:28.078000000.txt    4.735536499041592  count: 12308  percentage: 0.11027092889908256  node count: 651  edge count: 705\n",
      "index_count: 28\n",
      "thr: 5.498210744465611\n",
      "2018-04-10 13:49:28.078000000~2018-04-10 14:05:08.084000000.txt    6.187958454776783  count: 4122  percentage: 0.09361373546511628  node count: 449  edge count: 465\n",
      "index_count: 29\n",
      "thr: 4.73566226497068\n",
      "2018-04-10 14:05:08.084000000~2018-04-10 14:21:43.187000000.txt    5.778987234395547  count: 1979  percentage: 0.08784623579545454  node count: 313  edge count: 315\n",
      "index_count: 30\n",
      "thr: 2.5979225207788685\n",
      "2018-04-10 14:21:43.187000000~2018-04-10 14:36:57.738000000.txt    4.3199773306669  count: 6456  percentage: 0.06499677835051547  node count: 242  edge count: 296\n",
      "index_count: 31\n",
      "thr: 5.557573856721288\n",
      "2018-04-10 14:36:57.738000000~2018-04-10 14:54:56.906000000.txt    6.317426322247951  count: 1665  percentage: 0.10162353515625  node count: 448  edge count: 448\n",
      "index_count: 32\n",
      "thr: 2.4833613454269567\n",
      "2018-04-10 14:54:56.906000000~2018-04-10 15:28:27.687000000.txt    4.156522518667019  count: 7257  percentage: 0.06623284170560748  node count: 399  edge count: 451\n",
      "index_count: 33\n",
      "thr: 5.624776435597058\n",
      "2018-04-10 15:28:27.687000000~2018-04-10 15:51:57.466000000.txt    6.189451683551055  count: 2302  percentage: 0.07493489583333333  node count: 520  edge count: 519\n",
      "index_count: 34\n",
      "thr: 5.191476128432947\n",
      "2018-04-10 15:51:57.466000000~2018-04-10 16:10:37.208000000.txt    5.976385175074384  count: 682  percentage: 0.083251953125  node count: 173  edge count: 172\n",
      "index_count: 35\n",
      "thr: 5.031098393453394\n",
      "2018-04-10 16:10:37.208000000~2018-04-10 16:25:54.701000000.txt    5.886899037395002  count: 2715  percentage: 0.09142645474137931  node count: 327  edge count: 370\n",
      "index_count: 36\n",
      "thr: 4.59308344048564\n",
      "2018-04-10 16:25:54.701000000~2018-04-10 16:41:04.776000000.txt    5.678561841976898  count: 3913  percentage: 0.0888671875  node count: 398  edge count: 410\n",
      "index_count: 37\n",
      "thr: 4.7223561084521215\n",
      "2018-04-10 16:41:04.776000000~2018-04-10 16:56:25.853000000.txt    5.729854385860867  count: 7179  percentage: 0.10160495923913043  node count: 754  edge count: 768\n",
      "index_count: 38\n",
      "thr: 4.0419335569873995\n",
      "2018-04-10 16:56:25.853000000~2018-04-10 17:11:31.301000000.txt    5.482005885332828  count: 6281  percentage: 0.1115234375  node count: 472  edge count: 524\n",
      "index_count: 39\n",
      "thr: 4.479527349575738\n",
      "2018-04-10 17:11:31.301000000~2018-04-10 17:27:31.099000000.txt    5.576708473785655  count: 6142  percentage: 0.09674269153225806  node count: 742  edge count: 764\n",
      "index_count: 40\n",
      "thr: 5.280022472825582\n",
      "2018-04-10 17:27:31.099000000~2018-04-10 17:47:55.016000000.txt    6.282765599730053  count: 759  percentage: 0.0926513671875  node count: 149  edge count: 170\n",
      "index_count: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 5.546134804190945\n",
      "2018-04-10 17:47:55.016000000~2018-04-10 18:02:59.013000000.txt    6.3304543767580155  count: 2805  percentage: 0.0830078125  node count: 588  edge count: 592\n",
      "index_count: 42\n",
      "thr: 2.6105861400205956\n",
      "2018-04-10 18:02:59.013000000~2018-04-10 18:17:59.904000000.txt    4.466596271735992  count: 7153  percentage: 0.06181727046460177  node count: 259  edge count: 313\n",
      "index_count: 43\n",
      "thr: 3.1824512030713343\n",
      "2018-04-10 18:17:59.904000000~2018-04-10 18:38:58.139000000.txt    4.886428809113736  count: 3432  percentage: 0.09309895833333333  node count: 317  edge count: 357\n",
      "index_count: 44\n",
      "thr: 5.216784105427056\n",
      "2018-04-10 18:38:58.139000000~2018-04-10 19:00:34.975000000.txt    5.993087308308223  count: 1906  percentage: 0.08863467261904762  node count: 569  edge count: 569\n",
      "index_count: 45\n",
      "thr: 5.68611242847822\n",
      "2018-04-10 19:00:34.975000000~2018-04-10 19:20:44.840000000.txt    6.239018425803158  count: 523  percentage: 0.07296316964285714  node count: 188  edge count: 184\n",
      "index_count: 46\n",
      "thr: 6.022489817415298\n",
      "2018-04-10 19:20:44.840000000~2018-04-10 19:40:54.882000000.txt    6.51387443460479  count: 236  percentage: 0.0576171875  node count: 122  edge count: 119\n",
      "index_count: 47\n",
      "thr: 6.124556283383351\n",
      "2018-04-10 19:40:54.882000000~2018-04-10 20:01:07.769000000.txt    6.592701337220335  count: 240  percentage: 0.046875  node count: 134  edge count: 130\n",
      "index_count: 48\n",
      "thr: 5.11609773463681\n",
      "2018-04-10 20:01:07.769000000~2018-04-10 20:26:09.203000000.txt    5.924415118750079  count: 7062  percentage: 0.08729727056962025  node count: 1113  edge count: 1115\n",
      "index_count: 49\n",
      "thr: 5.696644208683872\n",
      "2018-04-10 20:26:09.203000000~2018-04-10 20:52:04.007000000.txt    6.646847819295325  count: 94  percentage: 0.091796875  node count: 30  edge count: 27\n",
      "index_count: 50\n",
      "thr: 4.844314698698382\n",
      "2018-04-10 20:52:04.007000000~2018-04-10 21:12:55.647000000.txt    5.466367983142685  count: 2498  percentage: 0.0938251201923077  node count: 482  edge count: 483\n",
      "index_count: 51\n",
      "thr: 5.3260755633595185\n",
      "2018-04-10 21:12:55.647000000~2018-04-10 21:38:13.511000000.txt    6.472119103320095  count: 334  percentage: 0.08154296875  node count: 96  edge count: 100\n",
      "index_count: 52\n",
      "thr: 4.082105389725939\n",
      "2018-04-10 21:38:13.511000000~2018-04-10 21:57:28.644000000.txt    5.161919112281945  count: 229  percentage: 0.11181640625  node count: 70  edge count: 66\n",
      "index_count: 53\n",
      "thr: 3.984120245839609\n",
      "2018-04-10 21:57:28.644000000~2018-04-10 22:17:10.628000000.txt    4.8753348883223655  count: 1106  percentage: 0.09818892045454546  node count: 82  edge count: 81\n",
      "index_count: 54\n",
      "thr: 5.3726328954953635\n",
      "2018-04-10 22:17:10.628000000~2018-04-10 22:44:13.982000000.txt    6.615696409218678  count: 85  percentage: 0.0830078125  node count: 27  edge count: 25\n",
      "index_count: 55\n",
      "thr: 5.075990031798623\n",
      "2018-04-10 22:44:13.982000000~2018-04-10 23:09:32.277000000.txt    5.671809421040076  count: 308  percentage: 0.10026041666666667  node count: 66  edge count: 64\n",
      "index_count: 56\n",
      "thr: 6.340441624065944\n",
      "2018-04-10 23:09:32.277000000~2018-04-10 23:59:32.865000000.txt    7.004630927810699  count: 37  percentage: 0.09511568123393316  node count: 9  edge count: 8\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_4_10=[]\n",
    "\n",
    "\n",
    "file_l=os.listdir(\"graph_4_10\")\n",
    "index_count=0\n",
    "for f_path in sorted(file_l):\n",
    "    f=open(\"graph_4_10/\"+f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "#     print(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10/\")\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_10.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_count: 0\n",
      "thr: 3.8528433621874196\n",
      "graph_4_11/2018-04-11 00:00:00.063000000~2018-04-11 02:00:00.161000000.txt    4.251545197476028  count: 60  percentage: 0.05859375  node count: 2  edge count: 1\n",
      "index_count: 1\n",
      "thr: 6.124797815114574\n",
      "graph_4_11/2018-04-11 02:00:00.161000000~2018-04-11 03:45:00.251000000.txt    6.676980704211265  count: 121  percentage: 0.1181640625  node count: 12  edge count: 11\n",
      "index_count: 2\n",
      "thr: 6.577611277290132\n",
      "graph_4_11/2018-04-11 03:45:00.251000000~2018-04-11 05:30:00.544000000.txt    7.15253937872022  count: 95  percentage: 0.0927734375  node count: 10  edge count: 9\n",
      "index_count: 3\n",
      "thr: 6.000338036017544\n",
      "graph_4_11/2018-04-11 05:30:00.544000000~2018-04-11 07:00:52.974000000.txt    6.636973128009692  count: 104  percentage: 0.1015625  node count: 12  edge count: 11\n",
      "index_count: 4\n",
      "thr: 4.57161915882941\n",
      "graph_4_11/2018-04-11 07:00:52.974000000~2018-04-11 07:25:16.408000000.txt    5.422541378044991  count: 182  percentage: 0.04443359375  node count: 53  edge count: 49\n",
      "index_count: 5\n",
      "thr: 4.9574803137972205\n",
      "graph_4_11/2018-04-11 07:25:16.408000000~2018-04-11 07:46:44.962000000.txt    6.040186967585997  count: 130  percentage: 0.0634765625  node count: 43  edge count: 41\n",
      "index_count: 6\n",
      "thr: 4.885714407929405\n",
      "graph_4_11/2018-04-11 07:46:44.962000000~2018-04-11 08:05:53.555000000.txt    5.897505656394668  count: 95  percentage: 0.0927734375  node count: 46  edge count: 44\n",
      "index_count: 7\n",
      "thr: 4.883620514365415\n",
      "graph_4_11/2018-04-11 08:05:53.555000000~2018-04-11 08:21:10.246000000.txt    5.891878092546124  count: 1740  percentage: 0.08091517857142858  node count: 160  edge count: 160\n",
      "index_count: 8\n",
      "thr: 4.80588092234779\n",
      "graph_4_11/2018-04-11 08:21:10.246000000~2018-04-11 08:43:29.852000000.txt    5.945911073957337  count: 184  percentage: 0.08984375  node count: 56  edge count: 51\n",
      "index_count: 9\n",
      "thr: 4.579940484372622\n",
      "graph_4_11/2018-04-11 08:43:29.852000000~2018-04-11 08:58:36.759000000.txt    5.758962033425187  count: 1162  percentage: 0.1134765625  node count: 142  edge count: 144\n",
      "index_count: 10\n",
      "thr: 5.148827582847201\n",
      "graph_4_11/2018-04-11 08:58:36.759000000~2018-04-11 09:15:23.460000000.txt    6.008745237160856  count: 7063  percentage: 0.08621826171875  node count: 955  edge count: 970\n",
      "index_count: 11\n",
      "thr: 5.221512579654076\n",
      "graph_4_11/2018-04-11 09:15:23.460000000~2018-04-11 09:30:40.459000000.txt    5.794410059243128  count: 702  percentage: 0.1142578125  node count: 75  edge count: 72\n",
      "index_count: 12\n",
      "thr: 4.66534450782734\n",
      "graph_4_11/2018-04-11 09:30:40.459000000~2018-04-11 09:45:41.202000000.txt    5.704250883013292  count: 5408  percentage: 0.10778061224489796  node count: 793  edge count: 887\n",
      "index_count: 13\n",
      "thr: 4.101718946708937\n",
      "graph_4_11/2018-04-11 09:45:41.202000000~2018-04-11 10:02:14.946000000.txt    5.50095147212322  count: 3227  percentage: 0.10504557291666666  node count: 300  edge count: 364\n",
      "index_count: 14\n",
      "thr: 5.045608317688638\n",
      "graph_4_11/2018-04-11 10:02:14.946000000~2018-04-11 10:21:29.924000000.txt    6.170763255592906  count: 3705  percentage: 0.08614676339285714  node count: 470  edge count: 506\n",
      "index_count: 15\n",
      "thr: 7.342508080787992\n",
      "graph_4_11/2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000.txt    8.02203487015582  count: 1277  percentage: 0.0075124717620481925  node count: 82  edge count: 76\n",
      "index_count: 16\n",
      "thr: 4.556893443478083\n",
      "graph_4_11/2018-04-11 10:36:41.077000000~2018-04-11 10:53:51.436000000.txt    5.757938371943495  count: 5684  percentage: 0.1047317216981132  node count: 422  edge count: 481\n",
      "index_count: 17\n",
      "thr: 7.422876550093787\n",
      "graph_4_11/2018-04-11 10:53:51.436000000~2018-04-11 11:11:03.735000000.txt    7.8703452901013735  count: 1968  percentage: 0.06005859375  node count: 38  edge count: 30\n",
      "index_count: 18\n",
      "thr: 5.882926225258122\n",
      "graph_4_11/2018-04-11 11:11:03.735000000~2018-04-11 11:27:06.742000000.txt    6.620462000110322  count: 3151  percentage: 0.06993519176136363  node count: 671  edge count: 691\n",
      "index_count: 19\n",
      "thr: 5.048169908056044\n",
      "graph_4_11/2018-04-11 11:27:06.742000000~2018-04-11 11:42:13.848000000.txt    6.224743119812999  count: 2354  percentage: 0.08514178240740741  node count: 442  edge count: 463\n",
      "index_count: 20\n",
      "thr: 6.564661721833506\n",
      "graph_4_11/2018-04-11 11:42:13.848000000~2018-04-11 11:59:50.272000000.txt    6.7494484526706  count: 6059  percentage: 0.11601945465686274  node count: 271  edge count: 281\n",
      "index_count: 21\n",
      "thr: 7.1138528976026425\n",
      "graph_4_11/2018-04-11 11:59:50.272000000~2018-04-11 12:15:10.640000000.txt    7.63394250585155  count: 83  percentage: 0.003117487980769231  node count: 29  edge count: 21\n",
      "index_count: 22\n",
      "thr: 6.566055592195758\n",
      "graph_4_11/2018-04-11 12:15:10.640000000~2018-04-11 12:30:15.769000000.txt    6.973406181818663  count: 155  percentage: 0.07568359375  node count: 45  edge count: 39\n",
      "index_count: 23\n",
      "thr: 9.427727736112683\n",
      "graph_4_11/2018-04-11 12:30:15.769000000~2018-04-11 12:45:30.888000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 24\n",
      "thr: 7.122935866838852\n",
      "graph_4_11/2018-04-11 12:45:30.888000000~2018-04-11 13:01:09.797000000.txt    7.39699589010918  count: 1920  percentage: 0.044642857142857144  node count: 67  edge count: 63\n",
      "index_count: 25\n",
      "thr: 6.885129672230402\n",
      "graph_4_11/2018-04-11 13:01:09.797000000~2018-04-11 13:16:29.838000000.txt    7.2910623423911165  count: 1768  percentage: 0.06394675925925926  node count: 352  edge count: 351\n",
      "index_count: 26\n",
      "thr: 7.426122741092247\n",
      "graph_4_11/2018-04-11 13:16:29.838000000~2018-04-11 13:31:30.828000000.txt    7.8987020740214  count: 1936  percentage: 0.05560661764705882  node count: 331  edge count: 331\n",
      "index_count: 27\n",
      "thr: 7.554799184499254\n",
      "graph_4_11/2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000.txt    8.171142266111628  count: 1512  percentage: 0.0421875  node count: 173  edge count: 171\n",
      "index_count: 28\n",
      "thr: 7.887375965444142\n",
      "graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt    8.215267188997132  count: 2366  percentage: 0.042787905092592594  node count: 119  edge count: 116\n",
      "index_count: 29\n",
      "thr: 7.605637010485668\n",
      "graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt    8.008388675012737  count: 634  percentage: 0.015478515625  node count: 188  edge count: 184\n",
      "index_count: 30\n",
      "thr: 7.702034670419568\n",
      "graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt    8.137389582920884  count: 1869  percentage: 0.044516958841463415  node count: 110  edge count: 106\n",
      "index_count: 31\n",
      "thr: 7.48760004004145\n",
      "graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt    8.088613899942482  count: 387  percentage: 0.010498046875  node count: 27  edge count: 24\n",
      "index_count: 32\n",
      "thr: 7.55723629278329\n",
      "graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt    8.407677348991422  count: 1432  percentage: 0.03995535714285714  node count: 51  edge count: 46\n",
      "index_count: 33\n",
      "thr: 7.49666815564475\n",
      "graph_4_11/2018-04-11 15:04:48.749000000~2018-04-11 15:23:04.703000000.txt    7.81568372529389  count: 655  percentage: 0.058149857954545456  node count: 98  edge count: 95\n",
      "index_count: 34\n",
      "thr: 9.443875034020367\n",
      "graph_4_11/2018-04-11 15:23:04.703000000~2018-04-11 15:39:01.167000000.txt    0.0  count: 0  percentage: 0.0  node count: 0  edge count: 0\n",
      "index_count: 35\n",
      "thr: 7.193193434484955\n",
      "graph_4_11/2018-04-11 15:39:01.167000000~2018-04-11 15:54:01.828000000.txt    7.5918239813908786  count: 1406  percentage: 0.049037388392857144  node count: 373  edge count: 369\n",
      "index_count: 36\n",
      "thr: 7.104070954267669\n",
      "graph_4_11/2018-04-11 15:54:01.828000000~2018-04-11 16:09:02.909000000.txt    7.436568818094607  count: 812  percentage: 0.04405381944444445  node count: 309  edge count: 307\n",
      "index_count: 37\n",
      "thr: 7.260801141298989\n",
      "graph_4_11/2018-04-11 16:09:02.909000000~2018-04-11 16:24:25.756000000.txt    7.855507520296799  count: 1840  percentage: 0.06417410714285714  node count: 336  edge count: 333\n",
      "index_count: 38\n",
      "thr: 6.972559124263061\n",
      "graph_4_11/2018-04-11 16:24:25.756000000~2018-04-11 16:44:13.639000000.txt    7.438010021676459  count: 2049  percentage: 0.07146344866071429  node count: 417  edge count: 420\n",
      "index_count: 39\n",
      "thr: 6.363572783638819\n",
      "graph_4_11/2018-04-11 16:44:13.639000000~2018-04-11 16:59:14.685000000.txt    7.017548223802695  count: 2470  percentage: 0.08614676339285714  node count: 569  edge count: 642\n",
      "index_count: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 6.445541199690253\n",
      "graph_4_11/2018-04-11 16:59:14.685000000~2018-04-11 17:14:26.172000000.txt    6.981723474621144  count: 2922  percentage: 0.09204889112903226  node count: 593  edge count: 643\n",
      "index_count: 41\n",
      "thr: 6.9268633321009325\n",
      "graph_4_11/2018-04-11 17:14:26.172000000~2018-04-11 17:29:29.020000000.txt    7.353442709708365  count: 1821  percentage: 0.05927734375  node count: 422  edge count: 419\n",
      "index_count: 42\n",
      "thr: 7.119725274681899\n",
      "graph_4_11/2018-04-11 17:29:29.020000000~2018-04-11 17:44:48.533000000.txt    7.463733758827958  count: 1966  percentage: 0.06399739583333333  node count: 502  edge count: 500\n",
      "index_count: 43\n",
      "thr: 7.273289023876872\n",
      "graph_4_11/2018-04-11 17:44:48.533000000~2018-04-11 17:59:53.338000000.txt    7.6412286630722175  count: 2175  percentage: 0.06851688508064516  node count: 481  edge count: 478\n",
      "index_count: 44\n",
      "thr: 4.950617802988729\n",
      "graph_4_11/2018-04-11 17:59:53.338000000~2018-04-11 18:16:39.616000000.txt    6.105846006236132  count: 8248  percentage: 0.11033818493150685  node count: 628  edge count: 874\n",
      "index_count: 45\n",
      "thr: 7.2730387062941055\n",
      "graph_4_11/2018-04-11 18:16:39.616000000~2018-04-11 18:32:06.970000000.txt    7.576752265615142  count: 1889  percentage: 0.057647705078125  node count: 491  edge count: 489\n",
      "index_count: 46\n",
      "thr: 6.9037603444305935\n",
      "graph_4_11/2018-04-11 18:32:06.970000000~2018-04-11 18:47:24.055000000.txt    7.457158341734443  count: 1959  percentage: 0.07085503472222222  node count: 413  edge count: 412\n",
      "index_count: 47\n",
      "thr: 6.847807557841576\n",
      "graph_4_11/2018-04-11 18:47:24.055000000~2018-04-11 19:02:37.294000000.txt    7.266075103692736  count: 2594  percentage: 0.07237723214285714  node count: 422  edge count: 423\n",
      "index_count: 48\n",
      "thr: 6.296128097418185\n",
      "graph_4_11/2018-04-11 19:02:37.294000000~2018-04-11 19:18:03.851000000.txt    6.905404032020585  count: 4284  percentage: 0.099609375  node count: 557  edge count: 583\n",
      "index_count: 49\n",
      "thr: 7.010718295464439\n",
      "graph_4_11/2018-04-11 19:18:03.851000000~2018-04-11 19:33:17.804000000.txt    7.491325665656206  count: 2055  percentage: 0.062713623046875  node count: 415  edge count: 414\n",
      "index_count: 50\n",
      "thr: 7.071529773926401\n",
      "graph_4_11/2018-04-11 19:33:17.804000000~2018-04-11 19:48:43.675000000.txt    7.442452389322605  count: 1740  percentage: 0.056640625  node count: 418  edge count: 417\n",
      "index_count: 51\n",
      "thr: 4.620795904628759\n",
      "graph_4_11/2018-04-11 19:48:43.675000000~2018-04-11 20:04:35.343000000.txt    5.8088836133505835  count: 7661  percentage: 0.10110061233108109  node count: 642  edge count: 917\n",
      "index_count: 52\n",
      "thr: 6.952853599426028\n",
      "graph_4_11/2018-04-11 20:04:35.343000000~2018-04-11 20:27:04.183000000.txt    7.4562349096445155  count: 799  percentage: 0.05573381696428571  node count: 344  edge count: 338\n",
      "index_count: 53\n",
      "thr: 6.831103933310419\n",
      "graph_4_11/2018-04-11 20:27:04.183000000~2018-04-11 20:46:59.749000000.txt    7.418831891836202  count: 2216  percentage: 0.06980846774193548  node count: 602  edge count: 603\n",
      "index_count: 54\n",
      "thr: 6.993752891190173\n",
      "graph_4_11/2018-04-11 20:46:59.749000000~2018-04-11 21:04:21.264000000.txt    7.51669101636177  count: 868  percentage: 0.04709201388888889  node count: 136  edge count: 132\n",
      "index_count: 55\n",
      "thr: 6.962093491576187\n",
      "graph_4_11/2018-04-11 21:04:21.264000000~2018-04-11 21:29:14.176000000.txt    7.313900266599985  count: 1003  percentage: 0.06121826171875  node count: 254  edge count: 254\n",
      "index_count: 56\n",
      "thr: 5.4909283388231325\n",
      "graph_4_11/2018-04-11 21:29:14.176000000~2018-04-11 21:44:15.839000000.txt    6.3909251317507145  count: 4309  percentage: 0.10019066220238096  node count: 814  edge count: 883\n",
      "index_count: 57\n",
      "thr: 6.954461789663553\n",
      "graph_4_11/2018-04-11 21:44:15.839000000~2018-04-11 21:59:40.869000000.txt    7.361118760159602  count: 2223  percentage: 0.08349609375  node count: 649  edge count: 649\n",
      "index_count: 58\n",
      "thr: 5.0245890468363\n",
      "graph_4_11/2018-04-11 21:59:40.869000000~2018-04-11 22:18:09.134000000.txt    6.286450043403854  count: 5057  percentage: 0.09876953125  node count: 686  edge count: 761\n",
      "index_count: 59\n",
      "thr: 7.1824534360477035\n",
      "graph_4_11/2018-04-11 22:18:09.134000000~2018-04-11 22:33:22.263000000.txt    7.589322801007195  count: 2186  percentage: 0.06099330357142857  node count: 377  edge count: 375\n",
      "index_count: 60\n",
      "thr: 6.8930148360211625\n",
      "graph_4_11/2018-04-11 22:33:22.263000000~2018-04-11 22:49:00.957000000.txt    7.221240701917021  count: 1736  percentage: 0.084765625  node count: 274  edge count: 277\n",
      "index_count: 61\n",
      "thr: 6.656130556171751\n",
      "graph_4_11/2018-04-11 22:49:00.957000000~2018-04-11 23:46:00.517000000.txt    7.148345287945846  count: 86  percentage: 0.083984375  node count: 27  edge count: 23\n"
     ]
    }
   ],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_4_11\")\n",
    "# node_IDF_4_4_7=torch.load(\"node_IDF_4_4-7\")\n",
    "node_IDF_4_4_7=torch.load(\"node_IDF\")\n",
    "y_data_4_11=[]\n",
    "df_list_4_11=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_4_11=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_11.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_11/\")\n",
    "\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],node_IDF_4_4_7, file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "#                 print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_4_11.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graph_4_11/2018-04-11 10:21:29.924000000~2018-04-11 10:36:41.077000000.txt']\n",
      "9.02203487015582\n",
      "['graph_4_11/2018-04-11 13:31:30.828000000~2018-04-11 13:46:38.658000000.txt']\n",
      "9.171142266111628\n",
      "['graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt']\n",
      "9.215267188997132\n",
      "['graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt']\n",
      "9.008388675012737\n",
      "['graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt']\n",
      "9.137389582920884\n",
      "['graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt']\n",
      "9.088613899942482\n",
      "['graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt']\n",
      "9.407677348991422\n"
     ]
    }
   ],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>9:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_thr():\n",
    "    np.seterr(invalid='ignore')\n",
    "    step=0.01\n",
    "    thr_list=torch.arange(-5,5,step)\n",
    "    \n",
    "    \n",
    "\n",
    "    precision_list=[]\n",
    "    recall_list=[]\n",
    "    fscore_list=[]\n",
    "    accuracy_list=[]\n",
    "    auc_val_list=[]\n",
    "    for thr in thr_list:\n",
    "        threshold=thr\n",
    "        y_prediction=[]\n",
    "        for i in y_test_scores:\n",
    "            if i >threshold:\n",
    "                y_prediction.append(1)\n",
    "            else:\n",
    "                y_prediction.append(0)\n",
    "        precision,recall,fscore,accuracy,auc_val=classifier_evaluation(y_test, y_prediction)   \n",
    "        precision_list.append(float(precision))\n",
    "        recall_list.append(float(recall))\n",
    "        fscore_list.append(float(fscore))\n",
    "        accuracy_list.append(float(accuracy))\n",
    "        auc_val_list.append(float(auc_val))\n",
    "\n",
    "    max_fscore=max(fscore_list)\n",
    "    max_fscore_index=fscore_list.index(max_fscore)\n",
    "    print(max_fscore_index)\n",
    "    print(\"max threshold:\",thr_list[max_fscore_index])\n",
    "    print('precision:',precision_list[max_fscore_index])\n",
    "    print('recall:',recall_list[max_fscore_index])\n",
    "    print('fscore:',fscore_list[max_fscore_index])\n",
    "    print('accuracy:',accuracy_list[max_fscore_index])    \n",
    "    print('auc:',auc_val_list[max_fscore_index])\n",
    "    \n",
    "        \n",
    "     # list 转 tensor   \n",
    "#     precision_list=torch.tensor(precision_list)   \n",
    "#     recall_list=torch.tensor(recall_list)   \n",
    "#     fscore_list=torch.tensor(fscore_list)   \n",
    "#     accuracy_list=torch.tensor(accuracy_list)   \n",
    "#     auc_val_list=torch.tensor(auc_val_list)   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # plt.scatter(attack_x, attack_y, s=20, c='r', label='Attack graph',marker='*')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "\n",
    "    plt.plot(thr_list,precision_list,color='red',label='precision',linewidth=2.0,linestyle='-')\n",
    "    plt.plot(thr_list,recall_list,color='orange',label='recall',linewidth=2.0,linestyle='solid')\n",
    "    plt.plot(thr_list,fscore_list,color='y',label='F-score',linewidth=2.0,linestyle='dashed')\n",
    "    plt.plot(thr_list,accuracy_list,color='g',label='accuracy',linewidth=2.0,linestyle='dashdot')\n",
    "    plt.plot(thr_list,auc_val_list,color='b',label='auc_val',linewidth=2.0,linestyle='dotted')\n",
    "    # '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "\n",
    "\n",
    "    # plt.scatter(turnovers, graph_loss, c=color)\n",
    "    plt.xlabel(\"Threshold\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Rate\", fontdict={'size': 16})\n",
    "    plt.title(\"Different evaluation Indicators by varying threshold value\", fontdict={'size': 12})\n",
    "    plt.legend(loc='best', fontsize=12, markerscale=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    # groundtruth, pred_value\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "#     tn+=100\n",
    "#     print(clf_name,\" : \")\n",
    "    print('tn:',tn)\n",
    "    print('fp:',fp)\n",
    "    print('fn:',fn)\n",
    "    print('tp:',tp)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)    \n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"fscore:\",fscore)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "    print(\"auc_val:\",auc_val)\n",
    "    return precision,recall,fscore,accuracy,auc_val\n",
    "\n",
    "def minmax(data):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    ans=[]\n",
    "    for i in data:\n",
    "        ans.append((i-min_val)/(max_val-min_val))\n",
    "    return ans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "y_pred=[]\n",
    "for i in labels:\n",
    "    y.append(labels[i])\n",
    "    y_pred.append(pred_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 112\n",
      "fp: 2\n",
      "fn: 0\n",
      "tp: 5\n",
      "precision: 0.7142857142857143\n",
      "recall: 1.0\n",
      "fscore: 0.8333333333333333\n",
      "accuracy: 0.9831932773109243\n",
      "auc_val: 0.9912280701754386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7142857142857143,\n",
       " 1.0,\n",
       " 0.8333333333333333,\n",
       " 0.9831932773109243,\n",
       " 0.9912280701754386)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count attack edge numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            'shared_files',\n",
    "        'csb.tracee.27331.27355',\n",
    "        'netrecon',\n",
    "#         '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "     \n",
    "#             'firefox',\n",
    "        '153.178.46.202',\n",
    "       '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        '140.57.183.17',\n",
    "      \n",
    "        \n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[\n",
    "    \n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 41.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▍                                                                         | 1/5 [00:00<00:02,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.224369305282448\n",
      "1.7753377734411298\n",
      "thr: 7.887375965444142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████▊                                                       | 2/5 [00:00<00:01,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.785547736087028\n",
      "1.88005951626576\n",
      "thr: 7.605637010485668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████▏                                    | 3/5 [00:01<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.880720439636739\n",
      "1.8808761538552192\n",
      "thr: 7.702034670419568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████▌                  | 4/5 [00:01<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.293990785885803\n",
      "1.4624061694370978\n",
      "thr: 7.48760004004145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.508894729719863\n",
      "2.0322277087089513\n",
      "thr: 7.55723629278329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from graphviz import Digraph\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import community.community_louvain as community_louvain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Some common path abstraction for visualization\n",
    "replace_dic={\n",
    " '/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "     '/data/data/org.mozilla.fennec_firefox_dev/files/':'/data/data/org.mozilla.fennec_firefox_dev/files/*',\n",
    "    '/system/fonts/':'/system/fonts/*',\n",
    "    '/data/data/com.android.email/cache/':'/data/data/com.android.email/cache/*',\n",
    "    '/data/data/com.android.email/files/':'/data/data/com.android.email/files/*',\n",
    "    'UNNAMED':'UNNAMED:*',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name\n",
    "\n",
    "\n",
    "# Users should manually put the detected anomalous time windows here\n",
    "attack_list = [\n",
    "        'graph_4_11/2018-04-11 13:46:38.658000000~2018-04-11 14:02:21.103000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:02:21.103000000~2018-04-11 14:18:19.001000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:18:19.001000000~2018-04-11 14:33:38.600000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:33:38.600000000~2018-04-11 14:49:05.326000000.txt',\n",
    "    'graph_4_11/2018-04-11 14:49:05.326000000~2018-04-11 15:04:48.749000000.txt',\n",
    "]\n",
    "\n",
    "original_edges_count = 0\n",
    "graphs = []\n",
    "gg = nx.DiGraph()\n",
    "count = 0\n",
    "for path in tqdm(attack_list):\n",
    "    if \".txt\" in path:\n",
    "        line_count = 0\n",
    "        node_set = set()\n",
    "        tempg = nx.DiGraph()\n",
    "        f = open(path, \"r\")\n",
    "        edge_list = []\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            edge_list.append(jdata)\n",
    "\n",
    "        edge_list = sorted(edge_list, key=lambda x: x['loss'], reverse=True)\n",
    "        original_edges_count += len(edge_list)\n",
    "\n",
    "        loss_list = []\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean = mean(loss_list)\n",
    "        loss_std = std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr = loss_mean + 1.5 * loss_std\n",
    "        print(\"thr:\", thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss'] > thr:\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),\n",
    "                               str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))), str(hashgen(replace_path_name(e['dstmsg']))),\n",
    "                            loss=e['loss'], srcmsg=e['srcmsg'], dstmsg=e['dstmsg'], edge_type=e['edge_type'],\n",
    "                            time=e['time'])\n",
    "\n",
    "\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "\n",
    "# Generate the candidate subgraphs based on community discovery results\n",
    "communities = {}\n",
    "max_partition = 0\n",
    "for i in partition:\n",
    "    if partition[i] > max_partition:\n",
    "        max_partition = partition[i]\n",
    "for i in range(max_partition + 1):\n",
    "    communities[i] = nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "    communities[partition[e[0]]].add_edge(e[0], e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0], e[1])\n",
    "\n",
    "\n",
    "# Define the attack nodes. They are **only be used to plot the colors of attack nodes and edges**.\n",
    "# They won't change the detection results.\n",
    "# Didn't add too much nodes for coloring. Most of the results are compared with the ground truth documentations manually\n",
    "def attack_edge_flag(msg):\n",
    "    attack_nodes = [\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/shared_files',\n",
    "        '/data/local/tmp',\n",
    "        'csb.tracee.27331.27355',\n",
    "        '/data/data/org.mozilla.fennec_firefox_dev/csb.tracee.27331.27355',\n",
    "        '111.82.111.27',\n",
    "        '166.199.230.185',\n",
    "        'glx_alsa_675',\n",
    "    ]\n",
    "    flag = False\n",
    "    for i in attack_nodes:\n",
    "        if i in str(msg):\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "# Plot and render candidate subgraph\n",
    "os.system(f\"mkdir -p ./graph_visual/\")\n",
    "graph_index = 0\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge = gg.edges[e]\n",
    "            srcnode = e['srcnode']\n",
    "            dstnode = e['dstnode']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if True:\n",
    "            # source node\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color = 'red'\n",
    "            else:\n",
    "                src_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['srcmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['srcmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), color=src_node_color,\n",
    "                     shape=src_shape)\n",
    "\n",
    "            # destination node\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color = 'red'\n",
    "            else:\n",
    "                dst_node_color = 'blue'\n",
    "            dot.node(name=str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=str(\n",
    "                replace_path_name(temp_edge['dstmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), color=dst_node_color,\n",
    "                     shape=dst_shape)\n",
    "\n",
    "            if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "                edge_color = 'red'\n",
    "            else:\n",
    "                edge_color = 'blue'\n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=temp_edge['edge_type'],\n",
    "                     color=edge_color)\n",
    "\n",
    "    dot.render(f'./graph_visual/subgraph_' + str(graph_index), view=False)\n",
    "    graph_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
