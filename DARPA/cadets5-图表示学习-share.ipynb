{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成每天的测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_clear_data.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_8=test_day_new(graph_5_8,\"./clear_data/graph_5_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_9=test_day_new(graph_5_9,\"./clear_data/graph_5_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_11=test_day_new(graph_5_11,\"./clear_data/graph_5_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_12=test_day_new(graph_5_12,\"./clear_data/graph_5_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_15=test_day_new(graph_5_15,\"./clear_data/graph_5_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_16=test_day_new(graph_5_16,\"./clear_data/graph_5_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_17=test_day_new(graph_5_17,\"./clear_data/graph_5_17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算IDF字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的代码使用jinyuan 优化过后的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 5-9  5-12的IDF值\n",
    "\n",
    "# share_node_IDF = mp.Manager().dict()\n",
    "\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_5_9/\"\n",
    "file_l=os.listdir(\"graph_5_9/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_5_10/\"\n",
    "file_l=os.listdir(\"graph_5_10/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_5_11/\"\n",
    "file_l=os.listdir(\"graph_5_11/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_5_12/\"\n",
    "file_l=os.listdir(\"graph_5_12/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                node_set.add(str(jdata['srcmsg']))\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                node_set.add(str(jdata['dstmsg'])) \n",
    "\n",
    "\n",
    "node_list=list(node_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 窗口关系建立函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF_5_9_12=torch.load(\"node_IDF_5_9-12\")\n",
    "def cal_set_rel(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if 'netflow' not in i and '/home/admin/' not in i and  '/proc/' not in i :\n",
    "        \n",
    "#         'netflow' not in i\n",
    "#         and 'usr' not in i and 'var' not in i\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "            if i in node_IDF_4_4_7.keys():\n",
    "                IDF4=node_IDF_4_4_7[i]\n",
    "            else:\n",
    "                IDF4=math.log(len(file_list_4_4_7)/(1))    \n",
    "            \n",
    "#             print(IDF)\n",
    "            if (IDF+IDF4)>9:\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "# def cal_set_rel_bak(s1,s2,file_list):\n",
    "#     new_s=s1 & s2\n",
    "#     count=0\n",
    "#     for i in new_s:\n",
    "# #     jdata=json.loads(i)\n",
    "#         if 'netflow' not in i \\\n",
    "#     and '/home/admin/' not in i \\\n",
    "#             and '/home/user/' not in i\\\n",
    "#             and '/tmp/' not in i\\\n",
    "#     and '/tmp/refload_pStageMem_log' not in i\\\n",
    "#             and 'com.' not in i:\n",
    "        \n",
    "# #         and '.dziauz.tinyflashlight' not in i \\\n",
    "# #             and '/data/system_ce/ not in i \\\n",
    "            \n",
    "# #         and 'usr' not in i and 'proc' not in i and '675' not in i and 'firefox' not in i and 'tmp' not in i and 'thunderbird' not in i\n",
    "# #         'netflow' not in i\n",
    "# #         and 'usr' not in i and 'var' not in i\n",
    "#             if i in node_IDF.keys():\n",
    "#                 IDF=node_IDF[i]\n",
    "#             else:\n",
    "#                 IDF=math.log(len(file_list)/(1))           \n",
    "                   \n",
    "# #             print(IDF)\n",
    "# #             print(len(file_list))\n",
    "#             if IDF>math.log(len(file_list)*0.9/(1))  :\n",
    "#                 print(\"node:\",i,\" IDF:\",IDF)\n",
    "#                 count+=1\n",
    "#     return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_include_key_word_bak(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "\n",
    "        '/home/admin/',\n",
    "        '/home/user/',\n",
    "         'proc',\n",
    "        '/tmp/',\n",
    "        '/var/spool/mqueue/',\n",
    "      \n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def cal_set_rel_bak(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "        if is_include_key_word_bak(i) is not True:\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))           \n",
    "                   \n",
    "            if (IDF)>math.log(len(file_list)*0.9/(1))  :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_5_15\")\n",
    "node_IDF=torch.load(\"node_IDF_5_9-12\")\n",
    "y_data_5_15=[]\n",
    "df_list_5_15=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_15=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_5_15=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"./clear_data/graph_5_15/\"\n",
    "file_l=os.listdir(\"./clear_data/graph_5_15/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_15.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"./clear_data/graph_5_15/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_15:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_15.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_5_15.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_list=[]\n",
    "for hl in history_list_5_15:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>1000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_5_16\")\n",
    "node_IDF=torch.load(\"node_IDF_5_9-12\")\n",
    "y_data_5_16=[]\n",
    "df_list_5_16=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_16=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"./clear_data/graph_5_16/\"\n",
    "file_l=os.listdir(\"./clear_data/graph_5_16/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_16.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"./clear_data/graph_5_16/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_16:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_16.append(temp_hq)\n",
    "    index_count+=1\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>100:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list_5_16:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>1000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF=torch.load(\"node_IDF_5_17\")\n",
    "node_IDF=torch.load(\"node_IDF_5_9-12\")\n",
    "y_data_5_17=[]\n",
    "df_list_5_17=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_17=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_5_17=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"./clear_data/graph_5_17/\"\n",
    "file_l=os.listdir(\"./clear_data/graph_5_17/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_17.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"./clear_data/graph_5_17/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_17:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_17.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_5_17.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list_5_17:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>10:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label={}   \n",
    "\n",
    "filelist = os.listdir(\"./clear_data/graph_5_15\")\n",
    "for f in filelist:\n",
    "\n",
    "    pred_label[\"./clear_data/graph_5_15/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"./clear_data/graph_5_16\")\n",
    "for f in filelist:\n",
    "\n",
    "    pred_label[\"./clear_data/graph_5_16/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"./clear_data/graph_5_17\")\n",
    "for f in filelist:\n",
    "    pred_label[\"./clear_data/graph_5_17/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "\n",
    "filelist = os.listdir(\"graph_5_15\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_15/\"+f]=0\n",
    "\n",
    "    \n",
    "filelist = os.listdir(\"graph_5_16\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_16/\"+f]=0\n",
    "\n",
    "    \n",
    "filelist = os.listdir(\"graph_5_17\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_17/\"+f]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    'graph_5_16/2019-05-16 09:20:32.093582942~2019-05-16 09:36:08.903494477.txt', \n",
    " 'graph_5_16/2019-05-16 09:36:08.903494477~2019-05-16 09:51:22.110949680.txt', \n",
    " 'graph_5_16/2019-05-16 09:51:22.110949680~2019-05-16 10:06:29.403713371.txt', \n",
    " 'graph_5_16/2019-05-16 10:06:29.403713371~2019-05-16 10:21:47.983513184.txt', \n",
    "    \n",
    "#  'graph_5_16/2019-05-16 20:32:27.570220441~2019-05-16 20:48:38.072848659.txt', \n",
    "#  'graph_5_16/2019-05-16 21:19:00.930018779~2019-05-16 21:34:46.231624861.txt', \n",
    "#  'graph_5_16/2019-05-16 21:34:46.231624861~2019-05-16 21:49:46.992678639.txt', \n",
    "#  'graph_5_16/2019-05-16 21:49:46.992678639~2019-05-16 22:06:14.950154813.txt', \n",
    "#  'graph_5_16/2019-05-16 22:06:14.950154813~2019-05-16 22:21:40.662702391.txt', \n",
    "#  'graph_5_16/2019-05-16 22:21:40.662702391~2019-05-16 22:36:45.602858389.txt', \n",
    "#  'graph_5_16/2019-05-16 22:36:45.602858389~2019-05-16 22:51:51.220035024.txt', \n",
    "#  'graph_5_16/2019-05-16 22:51:51.220035024~2019-05-16 23:07:16.890296254.txt', \n",
    "#  'graph_5_16/2019-05-16 23:07:16.890296254~2019-05-16 23:22:54.052353000.txt',\n",
    "#  并非所有包含Nginx的时间窗口都会被列为异常。  自从Nginx的后门被利用之后，之后它再次出现的活动我们无法确认是正常还是异常行为。在gt文档中没有明确指出后续的活动是异常行为因此我们将后面的Nginx的活动标记为正常。这将会导致kairos出现较高的误报。 \n",
    "   \n",
    "    'graph_5_17/2019-05-17 10:02:11.321524261~2019-05-17 10:17:26.881636687.txt', \n",
    " 'graph_5_17/2019-05-17 10:17:26.881636687~2019-05-17 10:32:38.131495470.txt', \n",
    " 'graph_5_17/2019-05-17 10:32:38.131495470~2019-05-17 10:48:02.091564015.txt'\n",
    "]\n",
    "\n",
    "\n",
    "# 结合 Rao的文档，对GT文档进行补充。检测出的准确率非常高。\n",
    "\n",
    "for i in attack_list:\n",
    "    labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计attack edge数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "           'nginx',\n",
    "#         '128.55.12.167',\n",
    "#          '4.21.51.250',\n",
    "#          'ocMain.py',\n",
    "        'python',\n",
    "#          '98.23.182.25',\n",
    "#         '108.192.100.31',\n",
    "        'hostname',\n",
    "        'whoami',\n",
    "#         'cat /etc/passwd',  \n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[\n",
    "    'graph_5_16/2019-05-16 09:20:32.093582942~2019-05-16 09:36:08.903494477.txt', \n",
    " 'graph_5_16/2019-05-16 09:36:08.903494477~2019-05-16 09:51:22.110949680.txt', \n",
    " 'graph_5_16/2019-05-16 09:51:22.110949680~2019-05-16 10:06:29.403713371.txt', \n",
    " 'graph_5_16/2019-05-16 10:06:29.403713371~2019-05-16 10:21:47.983513184.txt', \n",
    "    'graph_5_17/2019-05-17 10:02:11.321524261~2019-05-17 10:17:26.881636687.txt', \n",
    " 'graph_5_17/2019-05-17 10:17:26.881636687~2019-05-17 10:32:38.131495470.txt', \n",
    " 'graph_5_17/2019-05-17 10:32:38.131495470~2019-05-17 10:48:02.091564015.txt'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '/run/shm/':'/run/shm/*',\n",
    "#     '/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/':'/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/*',\n",
    "   '/home/admin/.cache/mozilla/firefox/':'/home/admin/.cache/mozilla/firefox/*',\n",
    "    '/home/admin/.mozilla/firefox':'/home/admin/.mozilla/firefox*',    \n",
    "    '/data/replay_logdb/':'/data/replay_logdb/*', \n",
    "    '/home/admin/.local/share/applications/':'/home/admin/.local/share/applications/*', \n",
    "    \n",
    "    '/usr/share/applications/':'/usr/share/applications/*', \n",
    "    '/lib/x86_64-linux-gnu/':'/lib/x86_64-linux-gnu/*',     \n",
    "    '/proc/':'/proc/*', \n",
    "     '/stat':'*/stat', \n",
    "    '/etc/bash_completion.d/':'/etc/bash_completion.d/*', \n",
    "    '/usr/bin/python2.7':'/usr/bin/python2.7/*', \n",
    "     '/usr/lib/python2.7':'/usr/lib/python2.7/*', \n",
    "'/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "    'UNNAMED':'UNNAMED *',\n",
    "    '/usr/ports/':'/usr/ports/*',\n",
    "    '/usr/home/user/test':'/usr/home/user/test/*',\n",
    "    '/tmp//':'/tmp//*',\n",
    "    '/home/admin/backup/':'/home/admin/backup/*',\n",
    "    '/home/admin/./backup/':'/home/admin/./backup/*',\n",
    "    '/usr/home/admin/./test/':'/usr/home/admin/./test/*',\n",
    "    '/usr/home/admin/test/':'/usr/home/admin/test/*',\n",
    "    '/home/admin/out':'/home/admin/out*',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "hash2msg={}\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1.5*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                \n",
    "                hash2msg[str(hashgen(replace_path_name(e['srcmsg'])))]=replace_path_name(e['srcmsg'])\n",
    "                hash2msg[str(hashgen(replace_path_name(e['dstmsg'])))]=replace_path_name(e['dstmsg'])\n",
    "                \n",
    "            #不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#                                 不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里没有加入太多染色节点，后期手动对比GT文档\n",
    "\n",
    "def attack_edge_flag(msg):\n",
    "    attack_edge_type=[\n",
    "        'nginx',\n",
    "       \n",
    "  \n",
    "    ]\n",
    "    flag=False\n",
    "    for i in attack_edge_type:\n",
    "        if i in msg:\n",
    "            flag=True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测结果统计分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿用其他数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg20]",
   "language": "python",
   "name": "conda-env-pyg20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
