{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成每天的测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass.pt\")\n",
    "memory,gnn, link_pred,neighbor_loader=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_8=test_day_new(graph_5_8,\"graph_5_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_9=test_day_new(graph_5_9,\"graph_5_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_11=test_day_new(graph_5_11,\"graph_5_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_12=test_day_new(graph_5_11,\"graph_5_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_14=test_day_new(graph_5_14,\"graph_5_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_5_15=test_day_new(graph_5_15,\"graph_5_15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算IDF字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的代码使用jinyuan 优化过后的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 5-9  5-12的IDF值\n",
    "\n",
    "# share_node_IDF = mp.Manager().dict()\n",
    "\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_5_8/\"\n",
    "file_l=os.listdir(\"graph_5_8/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_5_9/\"\n",
    "file_l=os.listdir(\"graph_5_9/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_5_11/\"\n",
    "file_l=os.listdir(\"graph_5_11/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "file_path=\"graph_5_12/\"\n",
    "file_l=os.listdir(\"graph_5_12/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']):\n",
    "                node_set.add(str(jdata['srcmsg']))\n",
    "            if 'netflow' not in str(jdata['dstmsg']):\n",
    "                node_set.add(str(jdata['dstmsg'])) \n",
    "\n",
    "\n",
    "node_list=list(node_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 窗口关系建立函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF\")\n",
    "\n",
    "\n",
    "def cal_set_rel_bak(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if 'netflow' not in i and '/data/system/' not in i \\\n",
    "            and '/storage/emulated/' not in i \\\n",
    "            and  '/data/data/com.android' not in i \\\n",
    "            and  '/proc/' not in i \\\n",
    "            and '/sys/devices/' not in i \\\n",
    "            and 'org.mozilla.fennec_vagrant' not in i \\\n",
    "            and 'mark.via.gp' not in i \\\n",
    "            and '/data/system_ce/' not in i \\\n",
    "            and '/Camera' not in i \\\n",
    "            and 'kohimovie.info.kohimovies' not in i \\\n",
    "            and '.dziauz.tinyflashlight' not in i \\\n",
    "            and 'com.' not in i:\n",
    "        #             and 'screencap' not in i \\\n",
    "        \n",
    "#         and '.dziauz.tinyflashlight' not in i \\\n",
    "#             and '/data/system_ce/ not in i \\\n",
    "            \n",
    "#         and 'usr' not in i and 'proc' not in i and '675' not in i and 'firefox' not in i and 'tmp' not in i and 'thunderbird' not in i\n",
    "#         'netflow' not in i\n",
    "#         and 'usr' not in i and 'var' not in i\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))           \n",
    "                   \n",
    "#             print(IDF)\n",
    "#             print(len(file_list))\n",
    "            if IDF>math.log(len(file_list)*0.9/(1))  :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF\")\n",
    "y_data_5_14=[]\n",
    "df_list_5_14=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_14=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"graph_5_14/\"\n",
    "file_l=os.listdir(\"graph_5_14/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_14.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_5_14/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_14:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_14.append(temp_hq)\n",
    "    index_count+=1\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list_5_14:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>2000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF\")\n",
    "y_data_5_15=[]\n",
    "df_list_5_15=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_15=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "loss_list_5_15=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"graph_5_15/\"\n",
    "file_l=os.listdir(\"graph_5_15/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_15.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_5_15/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_15:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_15.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_5_15.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_list=[]\n",
    "for hl in history_list_5_15:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>2000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测5-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF\")\n",
    "y_data_5_17=[]\n",
    "df_list_5_17=[]\n",
    "# node_set_list=[]\n",
    "history_list_5_17=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "loss_list_5_17=[]\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "file_path=\"graph_5_17/\"\n",
    "file_l=os.listdir(\"graph_5_17/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "index_count=0\n",
    "for f_path in (file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_5_17.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_5_17/\")\n",
    "    # 为当前窗口提取相关信息 用于和历史窗口进行对比\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list_5_17:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list_5_9_12)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                # 判断两个窗口之间是否存在交集\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "            if added_que_flag:\n",
    "                break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list_5_17.append(temp_hq)\n",
    "    index_count+=1\n",
    "    loss_list_5_17.append(loss_avg)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "for hl in history_list_5_17:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "#     name_list=[]\n",
    "    if loss_count>2000:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name']) \n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "pred_label={}\n",
    "    \n",
    "filelist = os.listdir(\"graph_5_14\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_14/\"+f]=0\n",
    "    pred_label[\"graph_5_14/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_5_15\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_15/\"+f]=0\n",
    "    pred_label[\"graph_5_15/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_5_17\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_5_17/\"+f]=0\n",
    "    pred_label[\"graph_5_17/\"+f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "'graph_5_15/2019-05-15 14:07:59.753000000~2019-05-15 14:23:00.265000000.txt',\n",
    "'graph_5_15/2019-05-15 14:23:00.265000000~2019-05-15 14:38:02.135000000.txt',\n",
    "    'graph_5_15/2019-05-15 15:38:59.175000000~2019-05-15 15:55:38.955000000.txt', \n",
    "'graph_5_15/2019-05-15 15:55:38.955000000~2019-05-15 16:11:27.687000000.txt',    \n",
    "#      'graph_5_17/2019-05-17 14:50:52.897000000~2019-05-17 15:06:00.371000000.txt', \n",
    "     'graph_5_17/2019-05-17 15:06:00.371000000~2019-05-17 15:21:40.474000000.txt', \n",
    "     'graph_5_17/2019-05-17 15:21:40.474000000~2019-05-17 15:36:41.964000000.txt', \n",
    "    'graph_5_17/2019-05-17 15:36:41.964000000~2019-05-17 15:51:43.493000000.txt',\n",
    "     'graph_5_17/2019-05-17 15:51:43.493000000~2019-05-17 16:06:44.953000000.txt', \n",
    "     'graph_5_17/2019-05-17 16:21:46.485000000~2019-05-17 16:36:47.843000000.txt', \n",
    "    'graph_5_17/2019-05-17 16:36:47.843000000~2019-05-17 16:51:49.352000000.txt'\n",
    "#     'graph_5_17/2019-05-17 16:36:47.843000000~2019-05-17 16:51:49.352000000.txt'\n",
    "]\n",
    "\n",
    "# graph_5_17/2019-05-17 11:48:07.561000000~2019-05-17 12:03:08.991000000.txt  # 可能是异常\n",
    "for i in labels:\n",
    "    if i in attack_list:\n",
    "        labels[i]=1\n",
    "    else:\n",
    "        labels[i]=0\n",
    "\n",
    "# for i in attack_list:\n",
    "#     labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计attack edge数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            'barephone-instr.apk',\n",
    "        'screencap-instr.apk',\n",
    "           'de.belu.appstarter',\n",
    "        './run_webserver.sh',\n",
    "        'appstarter-instr.apk',\n",
    "        'screenshot.png',\n",
    "        'screenshot',\n",
    "        '/dev/msm_g711tlaw',\n",
    "        'com.android.providers.contacts',\n",
    "        'barephone',\n",
    "        'busybox',\n",
    "\n",
    "        \n",
    "        '/data/local/tmp',\n",
    "        'calllog.db',\n",
    "        'calendar.db',        \n",
    "        'external.db',\n",
    "        'internal.db',\n",
    "        'lastAccess.db',\n",
    "        'mmssms.db',\n",
    "        \n",
    "\n",
    "#         '77.138.117.150',      \n",
    "#         '128.55.12.33',\n",
    "#         '128.55.12.233',\n",
    "#         '128.55.12.166',\n",
    "#         '49.8.46.240',\n",
    "#         '42.183.7.162',\n",
    "#         '133.39.25.45', \n",
    "        ]\n",
    "\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[]\n",
    "\n",
    "filelist = os.listdir(\"graph_5_15\")\n",
    "for f in filelist:    \n",
    "    files.append(\"graph_5_15/\"+f) \n",
    "    \n",
    "filelist = os.listdir(\"graph_5_17\")\n",
    "for f in filelist:\n",
    "    files.append(\"graph_5_17/\"+f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic={\n",
    "    '/run/shm/':'/run/shm/*',\n",
    "#     '/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/':'/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/*',\n",
    "   '/home/admin/.cache/mozilla/firefox/':'/home/admin/.cache/mozilla/firefox/*',\n",
    "    '/home/admin/.mozilla/firefox':'/home/admin/.mozilla/firefox*',    \n",
    "    '/data/replay_logdb/':'/data/replay_logdb/*', \n",
    "    '/home/admin/.local/share/applications/':'/home/admin/.local/share/applications/*', \n",
    "    \n",
    "    '/usr/share/applications/':'/usr/share/applications/*', \n",
    "    '/lib/x86_64-linux-gnu/':'/lib/x86_64-linux-gnu/*',     \n",
    "    '/proc/':'/proc/*', \n",
    "     '/stat':'*/stat', \n",
    "    '/etc/bash_completion.d/':'/etc/bash_completion.d/*', \n",
    "    '/usr/bin/python2.7':'/usr/bin/python2.7/*', \n",
    "     '/usr/lib/python2.7':'/usr/lib/python2.7/*', \n",
    "'/data/data/org.mozilla.fennec_firefox_dev/cache/':'/data/data/org.mozilla.fennec_firefox_dev/cache/*',\n",
    "    'UNNAMED':'UNNAMED *',\n",
    "}\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_edges_count=0\n",
    "\n",
    "graphs=[]\n",
    "gg=nx.DiGraph()\n",
    "count=0\n",
    "# file_list=os.listdir(\"./test_day_data4_10_emb100/\")\n",
    "for path in tqdm(attack_list):\n",
    "#     print(path)\n",
    "    if \".txt\" in path:\n",
    "        line_count=0\n",
    "        node_set=set()\n",
    "        tempg=nx.DiGraph()\n",
    "        f=open(path,\"r\")       \n",
    "        edge_list=[]\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            l=line.strip()\n",
    "            jdata=eval(l)\n",
    "#             temp_key=jdata['srcmsg']+jdata['dstmsg']+jdata['edge_type']\n",
    "#             if temp_key in train_edge_set:\n",
    "#                 jdata['loss']=(jdata['loss']-train_edge_set[temp_key]) if jdata['loss']>=train_edge_set[temp_key] else 0  \n",
    "#             jdata['loss']=abs(jdata['loss']-train_edge_set[temp_key])  if temp_key in train_edge_set else jdata['loss']\n",
    "            edge_list.append(jdata)\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True) \n",
    "        original_edges_count+=len(edge_list)\n",
    "        \n",
    "        loss_list=[]\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean=mean(loss_list)\n",
    "        loss_std=std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr=loss_mean+1*loss_std\n",
    "#         thr=-99\n",
    "        print(\"thr:\",thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss']>thr:    \n",
    "#             if True:  \n",
    "#                 if \"'/home/admin/profile'\" in e['srcmsg'] or \" '/home/admin/profile'\" in e['dstmsg']:\n",
    "#                     print(e['srcmsg'])\n",
    "#                     print(e['dstmsg'])\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),str(hashgen(replace_path_name(e['dstmsg']))),loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "                #不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "        print(path)\n",
    "        print(\"tempg edges:\",len(tempg.edges))\n",
    "        print(\"tempg nodes:\",len(tempg.nodes))\n",
    "        print(\"tempg weakly components:\",nx.number_weakly_connected_components(tempg))\n",
    "        \n",
    "        print(\"gg edges:\",len(gg.edges))\n",
    "        print(\"gg nodes:\",len(gg.nodes))\n",
    "        print(\"gg weakly components:\",nx.number_weakly_connected_components(gg))\n",
    "        print(f\"{original_edges_count=}\")\n",
    "#                                 不去除重复节点\n",
    "#                 gg.add_edge(e['srcnode'],e['dstnode'],loss=e['loss'],srcmsg=e['srcmsg'],dstmsg=e['dstmsg'],edge_type=e['edge_type'],time=e['time'])\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "#         print(path,\" line_count:\",line_count,\"  nodes count:\",len(node_set))\n",
    "                \n",
    "                \n",
    "                #         graphs.append(g)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_edge_flag(msg):\n",
    "    attack_edge_type=[\n",
    "    'barephone-instr.apk',\n",
    "        'screencap-instr.apk',\n",
    "           'de.belu.appstarter',\n",
    "        './run_webserver.sh',\n",
    "        'appstarter-instr.apk',\n",
    "        'screenshot.png',\n",
    "        'screenshot',\n",
    "        '/dev/msm_g711tlaw',\n",
    "        'com.android.providers.contacts',\n",
    "        'barephone',\n",
    "        'busybox',\n",
    "        'screencap',\n",
    " \n",
    "        '/data/local/tmp',\n",
    "        'calllog.db',\n",
    "        'calendar.db',        \n",
    "        'external.db',\n",
    "        'internal.db',\n",
    "        'lastAccess.db',\n",
    "        'mmssms.db',\n",
    "    ]\n",
    "    flag=False\n",
    "    for i in attack_edge_type:\n",
    "        if i in msg:\n",
    "            flag=True\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测结果统计分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿用其他数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg20]",
   "language": "python",
   "name": "conda-env-pyg20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
