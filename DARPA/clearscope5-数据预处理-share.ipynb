{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import *\n",
    "import threading\n",
    "import networkx as nx\n",
    "import math\n",
    "# 这里只处理和分析了marple-1的数据，至于其他两个agent采集到的数据，涉及到的攻击较少，所以就不分析了\n",
    "filePath=\"/home/monk/datasets/e5-clearscope/clearscope-2/\"\n",
    "filelist = os.listdir(filePath)\n",
    "\n",
    "import hashlib\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest() \n",
    "\n",
    "# 时间戳转换函数\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int 秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['ta1-clearscope-2-e5-official-1.bin.4.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.18.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.23.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.26.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.8.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.2.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.16.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.14.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.14.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.3.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.33.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.7.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.24.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.11.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.30.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.29.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.22.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.18.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.15.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.10.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.21.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.3.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.25.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.33.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.21.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.25.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.1.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.30.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.16.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.29.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.26.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.25.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.13.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.16.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.4.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.2.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.32.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.17.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.20.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.1.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.9.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.10.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.9.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.17.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.32.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.6.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.26.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.10.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.14.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.32.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.20.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.5.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.8.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.23.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.9.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.3.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.4.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.1.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.27.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.7.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.17.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.24.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.11.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.22.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.12.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.24.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.6.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.23.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.6.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.33.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.19.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.13.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.28.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.2.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.34.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.28.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.20.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.22.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.12.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.31.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.27.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.19.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.5.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.12.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.31.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.11.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.15.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.31.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.8.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.29.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.18.json',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.19.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.15.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.7.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.13.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.28.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.5.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.21.json.1',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.27.json.2',\n",
    "#  'ta1-clearscope-2-e5-official-1.bin.30.json.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间戳转换函数\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int 纳秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int 秒时间戳\n",
    "    :return: datetime   格式为 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str 格式为  %Y-%m-%d %H:%M:%S   例如 2013-10-10 23:40:00\n",
    "    :return: 纳秒时间戳\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp=timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接postgessql 数据库，将数据存入数据库中\n",
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_e5_clearscope_dataset_db',\n",
    "                           user = 'postgres',\n",
    "                           password = '21',\n",
    "                           port = '5432'#一般是5432\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "# 创建一个cursor来执行数据库的操作\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=cur.execute(\"\"\"\n",
    "    delete from subject_node_table where 1=1;\n",
    "\"\"\")\n",
    "print(tt)\n",
    "connect.commit()  #需要手动提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#发生错误时需要回滚\n",
    "connect.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据库表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分别处理不同类型节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套接字节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历数据集\n",
    "netobjset=set()\n",
    "netobj2hash={}# \n",
    "datalist=[]\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "#                 pass\n",
    "                if \"avro.cdm20.NetFlowObject\" in line:\n",
    "#                     print(line)\n",
    "                    try:\n",
    "                        res=re.findall('NetFlowObject\":{\"uuid\":\"(.*?)\"(.*?)\"localAddress\":{\"string\":\"(.*?)\"},\"localPort\":{\"int\":(.*?)},\"remoteAddress\":{\"string\":\"(.*?)\"},\"remotePort\":{\"int\":(.*?)}',line)[0]\n",
    "\n",
    "                        nodeid=res[0]\n",
    "                        srcaddr=res[2]\n",
    "                        srcport=res[3]\n",
    "                        dstaddr=res[4]\n",
    "                        dstport=res[5]\n",
    "\n",
    "                        nodeproperty=srcaddr+\",\"+srcport+\",\"+dstaddr+\",\"+dstport # 只关注向哪里发起的网络流 合理么？\n",
    "#                         nodeproperty=dstaddr+\",\"+dstport # 只关注向哪里发起的网络流 合理么？\n",
    "                        hashstr=stringtomd5(nodeproperty)\n",
    "                        netobj2hash[nodeid]=[hashstr,nodeproperty]\n",
    "                        netobj2hash[hashstr]=nodeid\n",
    "                        netobjset.add(hashstr)\n",
    "                    except:\n",
    "                            # 228786个文件节点是没有路径的，那么关于这种事件应该如何处理呢？ 直接扔掉?\n",
    "                        pass\n",
    "#                     print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in netobj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[netobj2hash[i][0]]+netobj2hash[i][1].split(\",\"))\n",
    "\n",
    "#写入数据库\n",
    "\n",
    "\n",
    "sql = '''insert into netflow_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  #需要手动提交\n",
    "\n",
    "del netobj2hash\n",
    "del datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进程节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scusess_count=0\n",
    "fail_count=0\n",
    "subject_objset=set()\n",
    "subject_obj2hash={}# \n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "#             for line in tqdm(f): \n",
    "            for line in (f):\n",
    "                if \"schema.avro.cdm20.Subject\" in line:\n",
    "#                     print(line)\n",
    "                    subject_uuid=re.findall('avro.cdm20.Subject\":{\"uuid\":\"(.*?)\",(.*?)\"path\":\"(.*?)\"',line)\n",
    "#                \n",
    "                    try:\n",
    "#                         (subject_uuid[0][-1])\n",
    "                        subject_obj2hash[subject_uuid[0][0]]=subject_uuid[0][-1]\n",
    "                        scusess_count+=1\n",
    "                    except:\n",
    "                        try:\n",
    "                            subject_obj2hash[subject_uuid[0][0]]=\"null\"\n",
    "                        except:\n",
    "                            pass\n",
    "#                             print(line)\n",
    "#                         print(line)                        \n",
    "                        fail_count+=1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in subject_obj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[stringtomd5(subject_obj2hash[i]),subject_obj2hash[i]])\n",
    "        \n",
    "#写入数据库\n",
    "\n",
    "\n",
    "sql = '''insert into subject_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  #需要手动提交\n",
    "\n",
    "\n",
    "del netobj2hash\n",
    "del datalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_obj2hash={}\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"avro.cdm20.FileObject\" in line:\n",
    "#                     print(line)\n",
    "                    \n",
    "                    Object_uuid=re.findall('cdm20.FileObject\":{\"uuid\":\"(.*?)\",(.*?){\"map\":{\"path\":\"(.*?)\"',line) \n",
    "#                     Object_uuid=re.findall('FileObject\":{\"uuid\":\"(.*?)\",',line) \n",
    "                    try:\n",
    "#                         file_node.add(Object_uuid[0])\n",
    "                        file_obj2hash[Object_uuid[0][0]]=Object_uuid[0][2]\n",
    "                    except:\n",
    "                        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in file_obj2hash.keys():\n",
    "    if len(i)!=64 :\n",
    "        datalist.append([i]+[stringtomd5(file_obj2hash[i]),file_obj2hash[i]])\n",
    "\n",
    "# 写入数据库\n",
    "sql = '''insert into file_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  #需要手动提交\n",
    "del file_obj2hash\n",
    "del datalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理事件数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预先准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 生成node2id表的数据\n",
    "node_list={}\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from file_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:    \n",
    "    node_list[i[1]]=[\"file\",i[-1]]\n",
    "\n",
    "file_uuid2hash={}\n",
    "for i in records:\n",
    "    file_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################    \n",
    "sql=\"\"\"\n",
    "select * from subject_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    node_list[i[1]]=[\"subject\",i[-1]]\n",
    "\n",
    "subject_uuid2hash={}\n",
    "for i in records:\n",
    "    subject_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from netflow_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    \n",
    "    node_list[i[1]]=[\"netflow\",i[-2]+\":\"+i[-1]]\n",
    "\n",
    "net_uuid2hash={}\n",
    "for i in records:\n",
    "    net_uuid2hash[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list_database=[]\n",
    "node_index=0\n",
    "for i in node_list:\n",
    "    node_list_database.append([i]+node_list[i]+[node_index])\n",
    "    node_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写入数据库\n",
    "sql = '''insert into node2id\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, node_list_database,page_size=10000)\n",
    "connect.commit()  #需要手动提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建nodeid 与msg之间的构建  创建字典变量 nodeid2msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # 可以实现 nodeid 转 msg      node hash 转 nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  #0到 n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeid2msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_edge_type=[\n",
    "    'EVENT_CLOSE',\n",
    "    'EVENT_OPEN',\n",
    "    'EVENT_READ',\n",
    "    'EVENT_WRITE',\n",
    "     'EVENT_EXECUTE',\n",
    "    'EVENT_RECVFROM',\n",
    "    'EVENT_RECVMSG',\n",
    "    'EVENT_SENDMSG',\n",
    "    'EVENT_SENDTO',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "edge_type=set()\n",
    "reverse=[\"EVENT_READ\",\"EVENT_RECVFROM\",\"EVENT_RECVMSG\"]        \n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in (f):\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm20.Event\"' in line:\n",
    "#                     print(line)\n",
    "                    subject_uuid=re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm20.UUID\":\"(.*?)\"}',line)\n",
    "                    predicateObject_uuid=re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm20.UUID\":\"(.*?)\"}',line)\n",
    "                    if len(subject_uuid) >0 and len(predicateObject_uuid)>0:\n",
    "                        if subject_uuid[0] in subject_uuid2hash\\\n",
    "                        and (predicateObject_uuid[0] in file_uuid2hash or predicateObject_uuid[0] in net_uuid2hash):\n",
    "                            #开始处理事件数据\n",
    "                            relation_type=re.findall('\"type\":\"(.*?)\"',line)[0]\n",
    "                            time_rec=re.findall('\"timestampNanos\":(.*?),',line)[0]\n",
    "                            time_rec=int(time_rec) # 将时间转成秒为单位\n",
    "                            subjectId=subject_uuid2hash[subject_uuid[0]]\n",
    "                            if predicateObject_uuid[0] in file_uuid2hash:\n",
    "                                objectId=file_uuid2hash[predicateObject_uuid[0]]\n",
    "                            else:\n",
    "                                objectId=net_uuid2hash[predicateObject_uuid[0]]\n",
    "#                                 print(line)\n",
    "                            edge_type.add(relation_type)\n",
    "                            if relation_type in reverse:\n",
    "                                datalist.append([objectId,nodeid2msg[objectId],relation_type,subjectId,nodeid2msg[subjectId],time_rec])\n",
    "                            else:\n",
    "                                datalist.append([subjectId,nodeid2msg[subjectId],relation_type,objectId,nodeid2msg[objectId],time_rec])\n",
    "\n",
    "               \n",
    "     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel=filter_type=[\n",
    "   'EVENT_ACCEPT',\n",
    " 'EVENT_CLONE',\n",
    " 'EVENT_CLOSE',\n",
    " 'EVENT_CREATE_OBJECT',\n",
    " 'EVENT_EXECUTE',\n",
    " 'EVENT_OPEN',\n",
    " 'EVENT_READ',\n",
    " 'EVENT_RECVFROM',\n",
    " 'EVENT_SENDTO',\n",
    " 'EVENT_WRITE',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照event type 筛选出需要用的events\n",
    "datalist_new=[]\n",
    "for d in datalist:\n",
    "    if d[2] in filter_type:      \n",
    "        datalist_new.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写入数据库\n",
    "#事件数据插入数据库\n",
    "\n",
    "sql = '''insert into event_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist_new,page_size=10000)\n",
    "connect.commit()  #需要手动提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 属性编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from sklearn import preprocessing #导入用于数据标准化的模块\n",
    "import numpy as np\n",
    "# 将path 转成分层的list\n",
    "\n",
    "FH_string=FeatureHasher(n_features=16,input_type=\"string\")\n",
    "FH_dict=FeatureHasher(n_features=16,input_type=\"dict\")\n",
    "\n",
    "\n",
    "def path2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "def ip2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('.')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'.'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def subject2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def list2str(l):\n",
    "    s=''\n",
    "    for i in l:\n",
    "        s+=i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_msg_vec=[]\n",
    "node_msg_dic_list=[]\n",
    "for i in tqdm(nodeid2msg.keys()):\n",
    "    if type(i)==int:\n",
    "        if 'netflow' in nodeid2msg[i].keys():\n",
    "            higlist=['netflow']\n",
    "            higlist+=ip2higlist(nodeid2msg[i]['netflow'])\n",
    "            \n",
    "        if 'file' in nodeid2msg[i].keys():\n",
    "            higlist=['file']\n",
    "            higlist+=path2higlist(nodeid2msg[i]['file'])\n",
    "            \n",
    "#             print(higlist)\n",
    "        if 'subject' in nodeid2msg[i].keys():\n",
    "            higlist=['subject']\n",
    "            higlist+=subject2higlist(nodeid2msg[i]['subject'])\n",
    "        node_msg_dic_list.append(list2str(higlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=[]\n",
    "for i in tqdm(node_msg_dic_list):\n",
    "    vec=FH_string.transform([i]).toarray()\n",
    "    node2higvec.append(vec)\n",
    "    \n",
    "#100%|██████████| 139960/139960 [00:16<00:00, 8724.76it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=np.array(node2higvec).reshape([-1,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_ACCEPT',\n",
    " 'EVENT_ACCEPT': 1,\n",
    " 2: 'EVENT_CLONE',\n",
    " 'EVENT_CLONE': 2,\n",
    " 3: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 3,\n",
    " 4: 'EVENT_CREATE_OBJECT',\n",
    " 'EVENT_CREATE_OBJECT': 4,\n",
    " 5: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 5,\n",
    " 6: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 6,\n",
    " 7: 'EVENT_READ',\n",
    " 'EVENT_READ': 7,\n",
    " 8: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 8,\n",
    " 9: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 9,\n",
    " 10: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成edge type one-hot变量\n",
    "relvec=torch.nn.functional.one_hot(torch.arange(0, len(rel2id.keys())//2), num_classes=len(rel2id.keys())//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将不同的relation type转为 变量 rel2vec   通过该变量，可以将字符串的关系类型转成ont hot编码\n",
    "rel2vec={}\n",
    "for i in rel2id.keys():\n",
    "    if type(i) is not int:\n",
    "        rel2vec[i]= relvec[rel2id[i]-1]\n",
    "        rel2vec[relvec[rel2id[i]-1]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存结果变量\n",
    "torch.save(node2higvec,\"node2higvec\")\n",
    "torch.save(rel2vec,\"rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载节点特征字典，边特征字典数据\n",
    "node2higvec=torch.load(\"./node2higvec\")\n",
    "rel2vec=torch.load(\"./rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tqdm(range(8,18)):\n",
    "    start_timestamp=datetime_to_ns_time_US('2019-05-'+str(day)+' 00:00:00')\n",
    "    end_timestamp=datetime_to_ns_time_US('2019-05-'+str(day+1)+' 00:00:00')\n",
    "    sql=\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp_rec>'%s' and timestamp_rec<'%s'\n",
    "           ORDER BY timestamp_rec;\n",
    "    \"\"\"%(start_timestamp,end_timestamp)\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print('2019-05-'+str(day),\" events count:\",str(len(events)))\n",
    "    edge_list=[]\n",
    "    for e in events:\n",
    "        edge_temp=[int(e[1]),int(e[4]),e[2],e[5]]\n",
    "        if e[2] in rel2id:# 如果该边类型符合 过滤器，就加入到数据集中\n",
    "#         if True:\n",
    "            edge_list.append(edge_temp)\n",
    "    print('2019-05-'+str(day),\" edge list len:\",str(len(edge_list)))\n",
    "    # 保存数据集数据\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    for i in edge_list:\n",
    "        src.append(int(i[0]))\n",
    "        dst.append(int(i[1]))\n",
    "    #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "        msg.append(torch.cat([torch.from_numpy(node2higvec[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec[i[1]])] ))\n",
    "        t.append(int(i[3]))\n",
    "    if len(edge_list)>0:\n",
    "        dataset.src = torch.tensor(src)\n",
    "        dataset.dst = torch.tensor(dst)\n",
    "        dataset.t = torch.tensor(t)\n",
    "        dataset.msg = torch.vstack(msg)\n",
    "        dataset.src = dataset.src.to(torch.long)\n",
    "        dataset.dst = dataset.dst.to(torch.long)\n",
    "        dataset.msg = dataset.msg.to(torch.float)\n",
    "        dataset.t = dataset.t.to(torch.long)\n",
    "        torch.save(dataset, \"./temporal_graphs/graph_5_\"+str(day)+\".TemporalData.simple\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
